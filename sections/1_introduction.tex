Wikipedia---the free encyclopedia that anyone can edit---faces many challenges in maintaining the quality of its articles and sustaining the volunteer community of editors. The people behind the hundreds of different language versions of Wikipedia have long relied on automation, bots, expert systems, recommender systems, human-in-the-loop assisted tools, and machine learning to help moderate and manage content at massive scales. The issues around artificial intelligence in Wikipedia are as complex as those facing other large-scale user-generated content platforms like Facebook, Twitter, or YouTube, as well as traditional corporate and governmental organizations that must make and manage decisions at scale. And like in those organizations, Wikipedia's automated classifiers are raising new and old issues about truth, power, responsibility, openness, and representation.

Yet Wikipedia's approach to AI has long been different than in corporate or governmental contexts typically discussed in emerging fields like Fairness, Accountability, and Transparency in Machine Learning (FATML) or Critical Algorithms Studies (CAS). The volunteer community of editors has strong ideological principles of openness, decentralization, and consensus-based decision-making. The paid staff at the non-profit Wikimedia Foundation---which legally owns and operates the servers---are not tasked with making editorial decisions about content\footnote{Except in rare cases, such as content that violates U.S. law, see \url{http://enwp.org/WP:OFFICE}}. This is instead the responsibility of the volunteer community, where a self-selected set of developers build tools, bots, and advanced technologies in broad consultation with the community. Even though Wikipedia's longstanding socio-technical system of algorithmic governance is far more open, transparent, and accountable than most platforms operating at Wikipedia's scale, ORES\footnote{\url{https://ores.wikimedia.org} and \url{http://enwp.org/:mw:ORES}}, the system we present in this paper, pushes even further on the crucial issue of who is able to participate in the development and use of advanced technologies.

ORES represents several innovations in openness in machine learning, particularly in seeing openness as a socio-technical challenge that is as much about scaffolding support as it is about open-sourcing code and data. With ORES, volunteers can curate labeled training data from a variety of sources for a particular purpose, commission the production of a machine classifier based on particular approaches and parameters, and make this classifier available via an API which anyone can query to score any edit to a page---operating in real time on the Wikimedia Foundation's servers. Currently, 102 classifiers have been produced for 41 languages, classifying edits in real-time based on criteria like ``damaging / not damaging,'' ``good faith / bad faith,'' or a language-specific article quality scale. ORES intentionally does not seek to produce a single classifier to enforce a gold standard of quality, nor does it prescribe particular ways in which scores and classifications will be incorporated into fully automated bots and semi-automated editing interfaces. As we will describe in section~\ref{sec:design_rationale}, ORES was built as a kind of cultural probe \cite{hutchinson2003technology} to support an open-ended set of community efforts to re-imagine what machine learning in Wikipedia is and who it is for.

Open participation in machine learning is widely relevant to both researchers of user-generated content platforms and those working across open collaboration, social computing, machine learning, and critical algorithms studies. ORES implements several of the dominant recommendations for algorithmic system builders around transparency and community consent\cite{crawford2016algorithm,diakopoulos2015algorithmic,sandvig2014auditing}. We discuss practical scoio-technical considerations for what openness, accountability, and transparency mean in a large-scale, real-world user-generated content platform. Wikipedia is also an excellent space for work on FATML topics, as the broader Wikimedia community and the non-profit Wikimedia Foundation are founded on ideals of open, public participation. All of the work presented in this paper is publicly-accessible and open sourced, from the source code and training data to the community discussions about ORES. Unlike in other nominally `public` platforms where users often do not know their data is used for research purposes, Wikipedians have extensive discussions about using their archived activity for research, with established guidelines we followed.\footnote{See \url{http://enwp.org/WP:NOTLAB} and \url{http://enwp.org/WP:Ethically_researching_Wikipedia}} This project is part of a longstanding engagement with the volunteer communities which involves extensive community consultation, and the case studies research have been approved by a university IRB.

In this paper, we first review related literature around open algorithmic systems, then discuss the socio-technical context of Wikipedia that lead us to building ORES. We discuss the operation of ORES, highlighting innovations in algorithmic \emph{openness} and \emph{transparency}. We present case studies of ORES that illustrate how it has broadened participation in machine learning.  Finally, we conclude with a discussion of the issues raised by this work and identify future directions.
