Wikipedia -- the free encyclopedia that anyone can edit -- faces many challenges in maintaining the quality of its articles and sustaining the volunteer community of editors. The people behind the hundreds of different language versions of Wikipedia have long relied on automation, bots, expert systems, recommender systems, human-in-the-loop assisted tools, and machine learning to help moderate and manage content at massive scales. The issues around artificial intelligence in Wikipedia are as complex as those facing other large-scale user-generated content platforms like Facebook, Twitter, or YouTube, as well as traditional corporate and governmental organizations that must make and manage decisions at scale. And like in those organizations, Wikipedia's automated classifiers are raising new and old issues about truth, power, responsibility, openness, and representation.

Yet Wikipedia's approach to AI has long been different than in the corporate or governmental contexts typically discussed in emerging fields like Fairness, Accountability, and Transparency in Machine Learning (FATML) or Critical Algorithms Studies (CAS). The volunteer community of editors has strong ideological principles of openness, decentralization, and consensus-based decision-making. The paid staff at the non-profit Wikimedia Foundation---which legally owns and operates the servers---are not tasked with making editorial decisions about content\footnote{Except in rare cases, such as content that violates U.S. law, see http://enwp.org/WP:OFFICE}. This is instead the responsibility of the volunteer community, where a self-selected set of developers build tools, bots, and advance technologies in broad consultation with the community. Even though Wikipedia's longstanding socio-technical system of algorithmic governance is far more open, transparent, and accountable than most platforms operating at Wikipedia's scale, ORES, the system we present in this paper, pushes even further on the crucial issue of who is able to participate in the development and use of advance technologies.

ORES represents several innovations in openness in machine learning, particularly in seeing openness as a socio-technical challenge that is as much about scaffolding support as it is about open-sourcing code and data. With ORES, volunteers can curate labeled training data from a variety of sources for a particular purpose, produce a machine classifier based on particular approaches and parameters, and make this classifier available via an API which anyone can query to score any edit to a page -- operating in real time on the Wikimedia Foundation's servers. Currently, 78 classifiers have been produced for 37 languages, classifying edits in real time on criteria like ``damaging / not damaging'', ``good faith / bad faith'', or an ordinal quality scale. ORES intentionally does not seek to produce a single classifier to enforce a gold standard of quality, nor does it prescribe particular ways in which scores and classifications will be incorporated into fully automated bots and semi-automated editing interfaces. Instead, ORES was built as a kind of cultural probe to support an open-ended set of community efforts to reimagine what machine learning in Wikipedia is and who it is for.

\subsection{Audiences for this work}
The issue of open participation in machine learning raises many issues that are widely relevant to both researchers of peer production platforms like Wikipedia, as well as those working across CSCW, social computing, machine learning, and critical algorithms studies.

To researchers of CSCW systems, this paper discusses the design and role of a technical system that supports a novel type of collaborative meta-work, as ORES makes it possible for volunteers to produce and deploy machine learning classifiers that other editors can use to support a variety of collaborative work practices in an established community of practice. In this paper, we detail this system's design as it was built to align with the particular ways in which volunteers work in Wikipedia. We also describe how ORES has altered the meta-work of Wikimedia tool developers.

To the FATML/CAS communities, we are introducing an open-by-design advanced algorithmic platform that is widely used to maintain a critical information resource.  This platform and its context implement several of the dominant recommendations for algorithmic system builders around transparency and community consent.  Through the deployment of this system and subsequent design iterations, we are able to discuss novel practical considerations for what openness, accountability, and transparency mean in a large scale, real world system.

To algorithmic system-builders, we describe how we have approached key issues in developing a working, scalable, and robust system that matches the decentralized work practices of end-users in Wikipedia.  Some of these approaches apply well described techniques (e.g. distributed processing and caching) while are novel strategies for giving tool developers and their users flexibility over how to use ORES's algorithmic predictions (e.g. model interrogation and threshold optimization).

At one level, ORES is widely relevant to researchers across social computing platforms, but it has specific relevance for researchers of commons-based peer production platforms like Wikipedia. ORES was created in the context of Wikipedia's much-discussed issues around newcomer socialization and inclusion. Wikipedia's existing algorithmic infrastructure for supporting various kinds of decision-making has generally been built by volunteers focused on quality control -- removing spam, hate speech, vandalism, and low-quality content as soon as possible. Newcomer socialization has suffered from an apparent tradeoff as Wikipedians focused heavily on efficient quality management strategies\cite{halfaker2013rise, halfaker2014snuggle}. Peer production projects generally appear to struggle with balancing managing quality and participation\cite{teblunthuis2018revisiting}, and automation in Wikipedia has generally made it more difficult for newcomers.

Past work has attempted to directly intervene by making tools\cite{halfaker2014snuggle} and designing spaces\cite{morgan2013tea} that directly support newcomer socialization.  While some of these interventions have shown promise\cite{morgan2018evaluating}, technological hurdles to change have prevented a systemic re-adjustment of the problematic aspects of quality control processes\cite{halfaker2014snuggle}. In this paper, we describe a novel system that represents a critical intervention in this space.  ORES is an advanced algorithmic prediction service for Wikipedians that is designed to democratize the development of work process support tools to a wider audience. Unlike past work, our goal in the development of ORES is not to directly solve the quality/newcomer problem ourselves.  Instead, we seek to remove barriers to others -- to enable more people in the community of tool developers to experiment with novel strategies for managing quality and newcomer support.

\subsection{Genre: A systems paper and a work study paper}
This paper is a blend of two classic genres of CSCW scholarship, which reflects our dual intellectual lineages. Traditionally, systems papers introduce and implement a novel design. Because they make a new kind of abstract interaction possible, empirical evaluations of the system within the context of work practices are typically left for future work (e.g. \cite{resnick1994grouplens}). In some ways, this paper follows this systems approach, in that we have produced a novel system and describe it with a thoughtful design rationale and technical specifications. But this paper also incorporates elements of a classic CSCW case study of work practices, which often point out how the abstract design rationales of a system did not align with the local context in which it was used (e.g. \cite{star1994steps}). Our design rationale is deeply based on prior empirical and theoretical research into the specific practices of Wikipedia as a socio-technical system. Furthermore, our system serves as a kind of cultural probe in that we seek to elicit ideas about machine learning from our users, and we describe several case studies of the adoption of ORES and the reflective responses of our users.

By combining a situated design rationale, a description of the system, and case studies of work practices, this paper is mindful of issues long raised in HCI and CSCW around the gulf between systems developers, designers, and researchers (e.g. \cite{gentner1990good, dourish2006implications, grudin1988cscw}). We follow the lead of contemporary CSCW researchers like Irani et al.\cite{irani2013turkopticon} and provide a situated reflection of work practices and contexts in line with our system description. While this approach makes for a longer paper, it allows us to refer back to the ecological effects we hypothesize as part of our design rationale when discussing ORES' adoption patterns and our case studies.we

In this paper, we first review related literature around open algorithmic systems, then discuss the socio-technical context of Wikipedia and the design rationale that lead us to building ORES.  Next, we describe how we engineered the ORES system to match Wikipedian work practices -- including innovations we've made with regards to algorithmic \emph{openness} and \emph{transparency}.  Then we present a small set of case studies of interesting uses and critiques of ORES' predictions.  Finally, we conclude with a discussion of the issues raised by this work with our target audiences: CSCW researchers, FATML/CAS researchers, social-computing researchers, and algorithmic system-builders.
