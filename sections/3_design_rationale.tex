In this section, we discuss systemic mechanisms behind Wikipedia's socio-technical problems and how we as system builders make positive impact.  Past work demonstrated how Wikipedia's problems are systemic with no readily apparent system-level solutions. To responsibly use machine learning in addressing these problems, we examined how Wikipedia functions as a distributed system: how processes, policies, power, and software come together to make Wikipedia happen.

\leadin{Wikipedia as a genre ecology}  Unlike traditional mass-scale projects, Wikipedia's structure and processes are not centrally planned. Wikipedia's system is a heterogeneous assemblage of humans, practices, policies, and software.  This open system and its processes are dynamic, complex, and non-deterministic.

Genre ecologies is a theoretical framework we found useful in helping us account for the totality of factors and their relationships in Wikipedia, which is essential to building a system-level understanding of state and change processes.  A genre ecology is ``an interrelated group of genres (artifact types and the interpretive habits that have developed around them) used to jointly mediate the activities that allow people to accomplish complex objectives.''\cite{spinuzzi2000genre}. The genre ecology framework arose from observational research on how collaborators amend and repurpose existing officially-sanctioned tools --- e.g. written documents, technological interfaces --- and developed their own unofficial tools to supplement or circumvent official tools. This was needed in order to account for practical contingencies and emergent needs not anticipated, a longstanding concern in CSCW. In tracing the relationships among the set of official and unofficial tools used communities of practice, this literature helps conceptalize the relationships between tool genres and individual human actors, addressing issues of distributed agency, interdependency, rule formalization, and power dynamics in sociotechnical systems\cite{spinuzzi2003tracing}.

Morgan \& Zachry used genre ecologies to characterize the relationships between Wikipedia's official policies and ``essays:'' unofficial rules, best practices, and editing advice documents that are created by editors in order to contextualize, clarify, and contradict policies\cite{morgan2010negotiating}. In Wikipedia, essays and policies co-exist and interact. For example, the ``proper'' interpretation of Wikipedia's official Civility policy\footnote{\url{http://enwp.org/WP:CIVIL}} within a particular context may need to account for the guidance provided in related essays such as ``No Angry Mastodons''\footnote{\url{http://enwp.org/WP:MASTODON}}. In genre ecology terms, performing the work of enforcing civil behavior on Wikipedia is dynamically and contingently \emph{mediated} by the guidance provided in the official policy and the guidance provided in any related essays. Unofficial genres provide interpretive flexibility in the application of official rules to local circumstances as well as challenging and re-interpreting official ideologies and objectives.

Algorithmic systems also have a role in mediating the policy, values, and rules in social spaces as well\cite{lessig1999code} \todo{I'd rather cite classic CSCW on expert systems: Orlikowski, Zuboff, Heath and Luff, Suchman, Star}.  When looking at Wikipedia's articulation work through the genre ecology lens, bots and human-computation tools mediate the meaning of policies and how Wikipedia enacts quality controls. For example, Sinebot enforces the signature policy in certain ways but not others\cite{geiger2011lives}, and the ``Huggle'' vandal fighting tool enacts quality in Wikipedia as a task of rapidly distinguishing good from bad edits in a live stream of recent changes\cite{halfaker2014snuggle}).

\leadin{Wikipedia's problems with automated mediation} \todo{Condense more. This rehashes a story told in section 2.} Wikipedia has a long-standing problem with quality control.  In 2006, when Wikipedia was growing exponentially, the volunteers who managed quality control processes were overwhelmed and turned to software agents to help make their process more efficient\cite{halfaker2014snuggle}.  Their software focused narrowly on most important problem to them --- removing damage at all costs --- and formalized processes that were previously more flexible and contingent. As they scaffolded their quality control workflows and values in code, other forms of quality control and socialization were pushed to the margins\cite{halfaker2013rise}.  The result was a marked decline in the retention of new editors in Wikipedia and a new threat to the core values of the project.

Where does change come from in a distributed cognition system that emerged based on community needs and volunteer priorities, where problematic assumptions have been embedded in the mediation of policy and the design of software for over a decade?  Or maybe more generally, how does deep change take place in a genre ecology?

\leadin{Making change is complicated by the distributed nature}
\todo{Also addressed in previous lit review}
As previously discussed, several initiatives were created to improve Wikipedian socialization practices, inlcuding the Teahouse and outreach efforts like Inspire Campaigns\cite{morgan2015what}, which elicited ideas from contributors on the margins of the community. However, the process of quality control has remained largely unchanged.  This assemblage of mindsets, policies, practices, and software prioritizes quality/efficiency and does so effectively \cite{geiger2013levee}\cite{halfaker2014snuggle}.  To move beyond the current state of quality control, we need alternatives to the existing mode of seeing and acting within Wikipedia.

Instead of the tempting technical solutions to \emph{just need to fix quality control}, it is not at all apparent what better quality control would look like.  Even if we did, how does one cause systemic change in a distributed system like Wikipedia?  We draw from standpoint epistemology, specifically Harding and Haraway's concept of \emph{successors}\cite{haraway1988situated}\cite{harding1987feminism} in reflecting on the development of new software/process/policy components.  Past work has explored developing a successor view that prioritizes the standpoints of mentors in support of new editors in Wikipedia, rather than the standpoints of vandal fighters focused on the efficiency of quality control\cite{halfaker2014snuggle}\cite{geiger2014successor}. However, a single point rarely changes the direction of an entire conversation or the shape of an entire ecology, so change is still elusive.

From these efforts, we know there is general interest in balancing quality/efficiency and diversity/welcomingness more effectively.  So where are these designers who incorporate this expanded set of values?  How to we help them bring forward their alternatives?  How do we help them re-mediate Wikipedia's policies and values through their lens?  How do we support the development of more successors?

\leadin{Expanding the margins of the ecology}
Successors come from the margin: they represent non-dominant values and engage in the re-mediation of articulation\cite{mugar2017preserving}.  In our view, such successors are a primary means to change in an open genre ecology like Wikipedia.  For anyone looking to enact a new view of quality control into the designs of a software system, there is a high barrier to entry: the development of a realtime machine prediction model.  Without exception, all of the critical, high efficiency quality control systems that keep Wikipedia clean of vandalism and other damage employ a machine prediction model for highlighting the edits that are most likely to be bad. For example, Huggle\footnote{\url{http://enwp.org/WP:Huggle}} and STiki\footnote{\url{http://enwp.org/WP:STiki}} use a machine prediction models to highlight likely damaging edits for human reviews.  ClueBot NG\footnote{\url{http://enwp.org/User:ClueBot\_NG}} uses a machine prediction model to automatically revert edits that are highly likely to be damaging.  These automated tools and their users work to employ a multi-stage filter that quickly and efficiently addresses vandalism\cite{geiger2013levee}.

Wikipedians have long had extensive discussions and debates about the development of the thousands of relatively simple rule-based bots that are tasked with enforcing rules or supporting various tasks \cite{geiger2011lives}. In contrast, there are high barriers to entry around machine learning classification models for quality control, both in knowing how they work and how to develop and operate them at Wikipedia's scale.  Without these skills, it was not possible for the average Wikipedian to create an alternative view of what quality controls should be, while also accounting for efficiency and the need to scale.  Notably, one of the key interventions in this area that did do so was also built by a computer scientist\cite{halfaker2014snuggle}.

The result is a dominance of a certain type of individual: a computer scientist or software engineer, who, as the stereotype goes, works with an eye towards efficiency but has little interest in messy human interactions.  This high barrier to entry and in-group effects has exacerbated the minimization of the margin and a supreme dominance of the authority of quality control regimes that were largely developed in 2006---long before the social costs of efficient quality control were understood.

\leadin{Lowering the barriers to entry}
Wikipedia's quality control processes are open to the development of successor systems for re-mediating quality control, but only for those with the right skills. We have two options for expanding the margins: (1) increase general literacy around machine classification techniques and operations at scale; or (2) minimize the need to deeply understand practical machine learning at scale in order to develop an effective quality control tool.

The development of ORES is the second option.  By deploying a high-availability machine prediction service that supports multiple classifiers at scale, designing accessible interfaces to engage with such classifiers in various ways, and engaging in basic outreach efforts, we sought to dramatically lower the barriers to the development of successor systems. In lowering the barriers to alternative visions of what quality control and newcomer socialization in Wikipedia should look like, we also open the doors to participation of alternative views in the genre ecology around quality control.  For us, we measure success not through higher rates of precision and recall, but instead though the new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be.
