In this section, we discuss systemic mechanisms behind Wikipedia's socio-technical problems and how we as system builders designed ORES to have impact within Wikipedia.  Past work has demonstrated how Wikipedia's problems are systemic and caused in part to inherent biases in the system of quality control in Wikipedia. To responsibly use machine learning in addressing these problems, we examined how Wikipedia functions as a distributed system using the concept of genre ecologies, focusing on how processes, policies, power, and software come together to make Wikipedia happen.

\subsection{Our goal: Lowered barriers to participation}
For anyone looking to enact a new view of quality control into the designs of a software system, there is a high barrier to entry: the development of a real-time machine prediction model.  Without exception, all of the critical, high efficiency quality control systems that keep Wikipedia clean of vandalism and other damage employ a machine prediction model for highlighting the edits that are most likely to be bad. For example, Huggle and STiki\footnote{\url{http://enwp.org/WP:STiki}} use machine prediction models to highlight likely damaging edits for human reviews.  ClueBot NG\footnote{\url{http://enwp.org/User:ClueBot_NG}} uses a machine prediction model to automatically revert edits that are highly likely to be damaging.

Wikipedia's quality control processes have always been open for re-consideration, but only for those with the right skills and capacities. We have two options for expanding the margins of this conversation\cite{mugar2017preserving}: (1) increase general literacy around machine classification techniques and operations at scale; or (2) minimize the need to navigate the technicalities of machine learning at scale in order to experiment with and develop novel advanced algorithmic technologies.

Our goal in the development of ORES is to explore the second option.  By deploying a high-availability machine prediction service, designing accessible interfaces, and engaging in basic outreach efforts, we seek to dramatically lower the barriers to the development of new algorithmic systems that could implement radically new ideas about what should be classified, how it should be classified, and how classifications and scores should be used.

\subsection{Our measure of success: More voices}
We measure success not through higher rates of precision and recall (though we are, of course, interested in that as well), but instead though the new conversations about how algorithmic tools affect editing dynamics.  If ORES is a successful intervention, it will enable experimentation in Wikipedia's socio-technical \emph{conversations} about quality control.  This translates into the development of novel tools and serious, critical reflection on the roles that algorithms play in mediating Wikipedia's quality and newcomer support processes.

If instead, we only see the same discussions (e.g. "How do we make vandal fighting more efficient?") and similar tools focused on quality control to the exclusion of newcomer socialization, we'll know that we missed the mark.
