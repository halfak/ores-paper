In this section, we discuss systemic mechanisms behind Wikipedia's socio-technical problems and how we as system builders designed ORES to have impact within Wikipedia.  Past work has demonstrated how Wikipedia's problems are systemic and caused in part to inherent biases in the system of quality control. To responsibly use machine learning in addressing these problems, we examined how Wikipedia functions as a distributed system, focusing on how processes, policies, power, and software come together to make Wikipedia happen.

\subsection{The problem: Stagnation in quality control practices}
As discussed in the previous section, while there is an apparent need for re-engineering Wikipedia's quality control practices and many efforts to make improvements, the quality control processes that were designed over a decade ago remain largely unchanged~\cite{halfaker2014snuggle}.  Why is this work practice so hard to make adjustments to?

Like the rest of Wikipedia's system of processes, quality control policy and practice are open to redesign via a consensus\footnote{\url{https://enwp.org/WP:CONSENSUS}} conversation.  Historically, the people with the skills and inclination to develop software tools that support work processes in Wikipedia have held a large amount of power in deciding what types of work will and will not be supported~\cite{niederer2010wisdom,geiger2011lives,muller2013work,tkacz2014wikipedia,livingstone2016population}.  In theory, one promising strategy to change quality control practices is to develop tools that capture an alternative vision of what's important (e.g. focusing on newcomer socialization or supporting a diverse set of newcomers).  

However, building a software system that would be useful for quality control work for Wikipedia is very difficult.  Scale and efficiency are critical considerations in the work practice of quality control in Wikipedia.  English Wikipedia sees over 142K edits per day\footnote{\url{https://quarry.wmflabs.org/query/38370}}.  If a reviewer could check 1 revision every 5 seconds, it would require 192 labor hours \emph{per day} to check all edits for blatant vandalism, hoaxes, and mistakes --- and this rate would only involve a cursory check.  The consequences of not dealing with damaging edits quickly and efficiently are quite high.  For example, losing just one of the components of the current complex regime of counter-vandalism tools has resulted in periods of Wikipedia's history where vandalism gathered twice as many views on average before being reverted~\cite{geiger2013levee}.

Without exception, all of the critical, efficient quality control tools that help keep Wikipedia clean of vandalism and other damage employ a real-time machine prediction model for flagging the edits that are most likely to be damaging. For example, Huggle and STiki\footnote{\url{http://enwp.org/WP:STiki}} use machine prediction models to highlight likely damaging edits for human review, and ClueBot NG\footnote{\url{http://enwp.org/User:ClueBot_NG}} uses a machine prediction model to automatically revert edits that are highly likely to be damaging.  All of these tools were first developed during the exponential growth period in Wikipedia's history --- before the social issues in quality control dynamics were apparent~\cite{halfaker2013rise}. Despite recent work to improve support for newcomers, these same tools and the processes they support continue to remain dominant today.  

\subsection{Our goal: Lowered barriers to participation}
For anyone looking to enact a new view of quality control into the designs of a software tool, there is a high barrier to entry: they must have the technical competency to design, build and manage a real-time, multilingual machine classifier that operates at Wikipedia's scale.  This is a narrow set of technical skills and capacities that few of the volunteers in the Wikipedian community possess.  Even those with advanced skills in machine learning and data engineering often have day jobs that prevent them from investing the time necessary to maintain these systems~\cite{sculley2015hidden}. Given Wikipedia's open participation model but continual issues with diversity and inclusion, it is important to note that free time is not equitably distributed in society~\citep{Bianchi2010}. 

In past work, researchers sought to directly enact alternative visions of quality control in Wikipedian tools by developing new alternatives premised on different values~\cite{halfaker2014snuggle}. However, these have generally not been adopted, and so we see more potential in employing a margin-building strategy --- akin to Nelle Morten's concept of ``hearing to speech''\footnote{Here we were inspired by Bowker and Star's reference to Nelle's work in their seminal book "Sorting Things Out"\cite{bowker1999sorting}}\cite{morton1985journey}: ``Speaking first to be heard is power over. Hearing to bring forth speech is empowering.''  While past work sought to ``speak'' about how quality control should be, we seek to employ a different tactic: broaden the diversity of voices participating in the \emph{conversation} about quality control practices by enabling more people to experiment with designing, auditing, redesigning, and implementing automated tools. 

Through the development of ORES, we explore the possibility of expanding the margins of this conversation\cite{mugar2017preserving}.  We think that deploying a high-availability machine prediction service, designing accessible interfaces, and engaging in basic outreach efforts, we will be able to dramatically lower the barriers to the development of new algorithmic tools that could implement new ideas about what should be classified, how it should be classified, and how classifications and scores should be used.  

\subsection{Our measure of success: More voices}
Our goal in this intervention is to ``hear to speech'': to enable those who were not able to participate in the \emph{socio-technical} conversation about Wikipedia's quality control practices to more easily have a voice.  So unlike most machine learning projects, we measure success not through higher rates of precision and recall (though we are, of course, interested in that as well), but instead though the new conversations about how algorithmic tools affect editing dynamics.  If ORES is a successful intervention, it will enable experimentation in Wikipedia's socio-technical \emph{conversations} about quality control.  This translates into the development of novel tools and serious, critical reflection on the roles that algorithms play in mediating Wikipedia's quality and newcomer support processes. If we only see the same discussions (e.g. "How do we make vandal fighting more efficient?") and similar tools focused on quality control to the exclusion of newcomer socialization, we will know that we have missed the mark.