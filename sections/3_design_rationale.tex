In this section, we discuss systemic mechanisms behind Wikipedia's socio-technical problems and how we as system builders designed ORES to have impact within Wikipedia.  Past work demonstrated how Wikipedia's problems are systemic with no readily apparent system-level solutions. To responsibly use machine learning in addressing these problems, we examined how Wikipedia functions as a distributed system using the concept of genre ecologies, focusing on how processes, policies, power, and software come together to make Wikipedia happen.

\subsection{The context: Wikipedia as a genre ecology}
Unlike traditional mass-scale projects, Wikipedia's structure and processes are not centrally planned. Wikipedia's system is a heterogeneous assemblage of humans, practices, policies, and software.  This open system and its processes are dynamic, complex, and non-deterministic.

Genre ecologies is a theoretical framework we found useful in helping us account for the totality of factors and their relationships in Wikipedia, which is essential to building a system-level understanding of state and change processes.  A genre ecology is ``an interrelated group of genres (artifact types and the interpretive habits that have developed around them) used to jointly mediate the activities that allow people to accomplish complex objectives.''\cite{spinuzzi2000genre}. The genre ecology framework arose from observational research on how collaborators amend and repurpose existing officially-sanctioned tools (e.g. written documents and technological interfaces) and developed their own unofficial tools to supplement or circumvent official tools. This was needed in order to account for practical contingencies and emergent needs not anticipated, a longstanding concern in CSCW. In tracing the relationships among the set of official and unofficial tools used in communities of practice, this literature helps conceptalize the relationships between tool genres and individual human actors, addressing issues of distributed agency, interdependency, rule formalization, and power dynamics in socio-technical systems\cite{spinuzzi2003tracing}.

Morgan \& Zachry used genre ecologies to characterize the relationships between Wikipedia's official policies and ``essays'': unofficial rules, best practices, and editing advice documents that are created by editors in order to contextualize, clarify, and contradict policies\cite{morgan2010negotiating}. In Wikipedia, essays and policies co-exist and interact. For example, the ``proper'' interpretation of Wikipedia's official Civility policy\footnote{\url{http://enwp.org/WP:CIVIL}} within a particular context may need to account for the guidance provided in related essays such as ``No Angry Mastodons''\footnote{\url{http://enwp.org/WP:MASTODON}}. In genre ecology terms, performing the work of enforcing civil behavior on Wikipedia is dynamically and contingently \emph{mediated} by the guidance provided in the official policy and the guidance provided in any related essays. Unofficial genres provide interpretive flexibility in the application of official rules to local circumstances as well as challenging and re-interpreting official ideologies and objectives.

Algorithmic systems also have a role in mediating the policy, values, rules, and emergent practices in social spaces as well\cite{lessig1999code,suchman2007human,orlikowski2015algorithm}.  When looking at Wikipedia's articulation work through the genre ecology lens, bots and human-computation tools mediate the meaning of policies and how Wikipedia enacts quality management. For example, Sinebot enforces the signature policy in certain ways but not others\cite{geiger2011lives}, and the ``Huggle'' counter-vandalism tool enacts quality in Wikipedia as a task of rapidly distinguishing good from bad edits in a live stream of recent changes\cite{halfaker2014snuggle}).

\subsection{The problem: Wikipedia's problems with automated mediation}
As discussed in section~\ref{sec:related_work}, through the co-development of collaborative work processes and technologies to support them, Wikipedians traded soft, human newcomer socialization for hard, efficient quality control. Their software focused narrowly on most important problem to them---removing damage quickly---and formalized processes that were previously more flexible and contingent. As they scaffolded their quality control workflows and values in code, other forms of quality control and socialization were pushed to the margins.  The result was a marked decline in the retention of new editors in Wikipedia and a new threat to the core values of the project.

Where does change come from in a system of distributed cognition that emerged based on community needs and volunteer priorities, where problematic assumptions have been embedded in the mediation of policy and the design of software for over a decade?  Or maybe more generally, how does deep change take place in a genre ecology?

\subsection{The complication: Making change in an decentralized ecology}
As previously discussed, several initiatives were created to improve Wikipedian socialization practices, inlcuding the Teahouse and outreach efforts like Inspire Campaigns\cite{morgan2015what}, which elicited ideas from contributors on the margins of the community. However, the process of quality control has remained largely unchanged.  This assemblage of mindsets, policies, practices, and software prioritizes quality/efficiency and does so effectively \cite{geiger2013levee}\cite{halfaker2014snuggle} but at a cost.

Instead of pursuing the tempting technical solutions to \emph{just fix quality control}, it is not at all apparent what better quality control would look like.  Even if we did, how does one cause systemic change in a decentralized system like Wikipedia?  We draw from standpoint epistemology, specifically Sandra Harding and Donna Haraway's concept of \emph{successors}\cite{haraway1988situated}\cite{harding1987feminism}, which helps us reflect on the development of new software/process/policy components.  Past work has explored developing a successor view that prioritizes the standpoints of mentors in support of new editors in Wikipedia, rather than the standpoints of vandal fighters focused on the efficiency of quality control\cite{halfaker2014snuggle}\cite{geiger2014successor}. However, a single point rarely changes the direction of an entire conversation or the shape of an entire ecology, so change is still elusive.

From these efforts, we know there is general interest in balancing quality/efficiency and diversity/welcomingness more effectively.  So where are these designers who incorporate this expanded set of values?  How to we help them bring forward their alternatives?  How do we help them re-mediate Wikipedia's policies and values through their lens?  How do we support the development of more successors, who can build interfaces, tools, and bots based on different ideas of what machine learning is and what it should be used for?

\subsection{The goal: Expanding the margins of the ecology}
Successors come from the margin: they represent non-dominant values and engage in the re-mediation of articulation\cite{mugar2017preserving}.  In our view, such successors are a primary means to change in an open genre ecology like Wikipedia.  For anyone looking to enact a new view of quality control into the designs of a software system, there is a high barrier to entry: the development of a realtime machine prediction model.  Without exception, all of the critical, high efficiency quality control systems that keep Wikipedia clean of vandalism and other damage employ a machine prediction model for highlighting the edits that are most likely to be bad. For example, Huggle\footnote{\url{http://enwp.org/WP:Huggle}} and STiki\footnote{\url{http://enwp.org/WP:STiki}} use a machine prediction models to highlight likely damaging edits for human reviews.  ClueBot NG\footnote{\url{http://enwp.org/User:ClueBot_NG}} uses a machine prediction model to automatically revert edits that are highly likely to be damaging.  These automated tools and their users work to employ a multi-stage filter that quickly and efficiently addresses vandalism\cite{geiger2013levee}.

Wikipedians have long had extensive discussions and debates about the development of the thousands of relatively simple rule-based bots that are tasked with enforcing rules or supporting various tasks \cite{geiger2011lives}. In contrast, there are high barriers to entry around machine learning classification models for quality control, both in knowing how they work and how to develop and operate them at Wikipedia's scale.  Without these skills, it was not possible for the average Wikipedian to create an alternative view of what quality controls should be, while also accounting for efficiency and the need to scale.  Notably, one of the key interventions in this area that did do so was also built by a computer scientist\cite{halfaker2014snuggle}.

The result is a dominance of a certain type of individual: a computer scientist or software engineer with an eye towards improving the efficiency of quality control.  This high barrier to entry and in-group effect has exacerbated the minimization of the margin and a supreme dominance of the authority of quality control regimes that were largely developed in 2006---long before the social costs of efficient quality control were understood.  Worse, this barrier stands in the way of a key aspect of ecological health: diversity.  We believe this lack of diversity has limited the adaptive capacity of Wikipedia's process ecology around quality management this has lead to the well-documented, long-standing issues with newcomer socialization\cite{halfaker2013rise}.

\leadin{Our approach: Lowering the barriers to entry}
Wikipedia's quality control processes are open to the development of successor systems for re-mediating quality control, but only for those with the right skills and capacities, which are not evenly distributed. We have two options for expanding the margins: (1) increase general literacy around machine classification techniques and operations at scale; or (2) minimize the need to deeply understand practical machine learning at scale in order to develop an effective quality control tool.

The development of ORES is the second option.  By deploying a high-availability machine prediction service that supports multiple classifiers at scale, designing accessible interfaces to engage with such classifiers in various ways, and engaging in basic outreach efforts, we seek to dramatically lower the barriers to the development of new algorithmic systems that could implement radically new ideas about what should be classified, how it should be classified, and how classifications and scores should be used. In lowering the barriers to alternative visions of what quality control and newcomer socialization in Wikipedia should look like, we also open the doors to participation of alternative views in the genre ecology around quality control.  For us, we measure success not through higher rates of precision and recall, but instead though the new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be.
