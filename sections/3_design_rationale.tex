In this section, we discuss systemic mechanisms behind Wikipedia's socio-technical problems and how we as system builders designed ORES to have impact within Wikipedia.  Past work has demonstrated how Wikipedia's problems are systemic and caused in part to inherent biases in the system of quality control in Wikipedia. To responsibly use machine learning in addressing these problems, we examined how Wikipedia functions as a distributed system using the concept of genre ecologies, focusing on how processes, policies, power, and software come together to make Wikipedia happen.

\subsection{Making change in an decentralized ecology}
As previously discussed, several initiatives were created to improve Wikipedian socialization practices, inlcuding the Teahouse and outreach efforts like Inspire Campaigns\cite{morgan2015what}, which elicited ideas from contributors on the margins of the community. However, the process of quality control has remained largely unchanged.  This assemblage of mindsets, policies, practices, and software prioritizes quality/efficiency and does so effectively \cite{geiger2013levee}\cite{halfaker2014snuggle} but at a cost.

Instead of pursuing the tempting technical solutions to \emph{just fix quality control}, it is not at all apparent what better quality control would look like.  Even if we did, how does one cause systemic change in a decentralized system like Wikipedia?  We draw from standpoint epistemology, specifically Sandra Harding and Donna Haraway's concept of \emph{successors}\cite{haraway1988situated}\cite{harding1987feminism}, which helps us reflect on the development of new software/process/policy components.  Past work has explored developing a successor view that prioritizes the standpoints of mentors in support of new editors in Wikipedia, rather than the standpoints of vandal fighters focused on the efficiency of quality control\cite{halfaker2014snuggle}\cite{geiger2014successor}. However, a single point rarely changes the direction of an entire conversation or the shape of an entire ecology, so change is still elusive.

From these efforts, we know there is general interest in balancing quality/efficiency and diversity/welcomingness more effectively.  So where are these designers who incorporate this expanded set of values?  How do we help them bring forward their alternatives?  How do we help them re-mediate Wikipedia's policies and values through their lens?  How do we support the development of more successors, who can build interfaces, tools, and bots based on different ideas of what machine learning is and what it should be used for?

\subsection{Our goal: Expanding the margins for successors}
Successors come from the margin: they represent non-dominant values and engage in the re-mediation of articulation\cite{mugar2017preserving}.  In our view, such successors are a primary means to change in an open genre ecology like Wikipedia.  For anyone looking to enact a new view of quality control into the designs of a software system, there is a high barrier to entry: the development of a realtime machine prediction model.  Without exception, all of the critical, high efficiency quality control systems that keep Wikipedia clean of vandalism and other damage employ a machine prediction model for highlighting the edits that are most likely to be bad. For example, Huggle and STiki\footnote{\url{http://enwp.org/WP:STiki}} use machine prediction models to highlight likely damaging edits for human reviews.  ClueBot NG\footnote{\url{http://enwp.org/User:ClueBot_NG}} uses a machine prediction model to automatically revert edits that are highly likely to be damaging.  These automated tools and their users work to employ a multi-stage filter that quickly and efficiently addresses vandalism\cite{geiger2013levee}.

Wikipedians have long had extensive discussions and debates about the development of the thousands of relatively simple rule-based bots that are tasked with enforcing rules or supporting various tasks \cite{geiger2011lives}. In contrast, there are high barriers to entry around machine learning classification models for quality control, both in knowing how they work and how to develop and operate them at Wikipedia's scale.  Without these skills, it was not possible for the average Wikipedian to create an alternative view of what quality controls should be, while also accounting for efficiency and the need to scale.  Notably, one of the key interventions in this area that did do so was also built by a computer scientist\cite{halfaker2014snuggle}.

The result is a dominance of a certain type of individual: a computer scientist or software engineer with an eye towards improving the efficiency of quality control.  This high barrier to entry and in-group effect has exacerbated the minimization of the margin and a supreme dominance of the authority of quality control regimes that were largely developed in 2006---long before the social costs of efficient quality control were understood.  Worse, this barrier stands in the way of a key aspect of ecological health: diversity.  We believe this lack of diversity has limited the adaptive capacity of Wikipedia's process ecology around quality management this has lead to the well-documented, long-standing issues with newcomer socialization\cite{halfaker2013rise}.

\subsection{What success looks like: Lowering barriers}
Wikipedia's quality control processes are open to the development of successor systems for re-mediating quality control, but only for those with the right skills and capacities, which are not evenly distributed. We have two options for expanding the margins: (1) increase general literacy around machine classification techniques and operations at scale; or (2) minimize the need to navigate the technicalities of machine learning at scale in order to develop advanced algorithmic technologies.

Our goal in the development of ORES is to explore the second option.  By deploying a high-availability machine prediction service that supports multiple classifiers at scale, designing accessible interfaces to engage with such classifiers in various ways, and engaging in basic outreach efforts, we seek to dramatically lower the barriers to the development of new algorithmic systems that could implement radically new ideas about what should be classified, how it should be classified, and how classifications and scores should be used. By enabling alternative visions of what quality control and newcomer socialization in Wikipedia should look like, we also open the doors to participation of alternative views in the genre ecology around quality control.  For us, we measure success not through higher rates of precision and recall, but instead though the new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be.
