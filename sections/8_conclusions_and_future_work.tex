Designing for empowerment leads us in new directions.  Rather than running an experiment on an ``intervention'' (e.g. \cite{halfaker2014snuggle}), we're reducing barriers and encouraging a stalled ``conversation'' continue.  We see this as a ``hearing to speech'' ala Nell Morten\cite{morton1985journey} in contrast to ``speaking to be heard''.  By building the exact technology or process we think is right, we would be ``speaking to be heard'' and forcing our own views into the technological conversation about how quality is enacted in Wikipedia.  By stepping back and asking, ``What is preventing others from getting involved in this conversation?'' and lowering those barriers, we run the risk that the conversation will not go in the directions that we value.  It is a conscious choice we make to attempt to empower others rather than to assert ourselves, but we do not make this choice purely out of altruism.  It is impractical for one individual or small group to drive a conversation in Wikipedia in a productive direction.  Halfaker et al.'s Snuggle intervention\cite{halfaker2014snuggle} tried that strategy with limited success in past work.  We think it's time to hear to speech and see what others would like to contribute from their own standpoints.

When considering our design rationale, ORES is not quite a successor system.  It doesn't directly enact an alternative vision of how quality control should function in Wikipedia  It is a system for making the construction of successor technologies easier.  By solving for efficiency and letting others design for process, a new set of expanded standpoints can more easily be expressed.  Under another view, ORES is maybe a successor in that it enacts an alternative view of how algorithms should be made available to the populations they govern.  By keeping algorithms integrated into specific tools and keeping the operational details of the algorithm hidden, past developers enacted values of control and focused use of their creations.  By opening ORES as an algorithmic system, encouraging experimentation, and providing tools for interrogation of the algorithms themselves, we enact a set of values that embrace experimentation and the progress of mediation.

We do intend to help Wikipedia's social process form in ways that align with our values---the more complete balance of efficient quality control and newcomer support.  We want to see Wikipedia work better. By ``better'' we mean that more people who want to contribute will feel welcome to do so and will find their place in Wikipedia.  In this paper we provide no direct evaluation of newcomer retention, nor do we look for evidence of improved newcomer socialization.  Instead, we're targeting the early precursors of social change: ecological health and critical reflection.

\subsection{Ecological health}
In section~\ref{sec:adoption_patterns}, we show clear evidence of a boom in the ecosystem of quality control technologies.  Through the adoption of ORES, new tools have quickly gained relevance.  Many of these tools re-mediate the way that quality control happens in Wikipedia.  For example, consider Sage Ross's use of ORES' interrogability to support newcomers in developing articles that Wikipedians would find acceptable that was discussed in section~\ref{sec:adoption_patterns}.  By using a quality model to direct newcomers behavior, socialization may be improved at no cost to efficiency---maybe even an improvement from a reviewer's point of view since the system would reault in more structurally complete drafts.  This addition to the technological conversation around quality control represents a substantial shift from the focus on the post-publication boundary\cite{geiger2012defense} to a focus on pre-review training for newcomers.  While Sage's interface has yet to be widely adopted, it represents a key example of a new concept in the technological conversation in Wikipedia.

While ORES has been live for 3 years at the time of writing, the ecological intervention is still young.  "ORES" has only recently become commonly known by Wikimedia contributors and we're still building out basic support for the different Wikimedia language/project communities.  In order to see a fundimental shift in the Wikimedia's technical ecology, we'll likely need to continue observing for 1-2 more years still.  But the early signs of ecological health (heavy adoption and increased diversity of quality management technologies) are what we hope to see---long term and continual re-mediation of quality and community management policies, processes, and technologies. 

\subsection{Critical reflection}
In section~\ref{sec:case_studies}, we show evidence of critical reflection on the current processes and the role of algorithms in quality control.  We believe that the case studies that we describe both show that this reflection is taking place and that the wide proliferation of tools that provide surprising alternative uses of ORES suggest that Wikipedians feel a renewed power over the their quality control processes.  Even if we are wrong about the direction of change that Wikipedia needs for quality control and newcomer socialization, we're inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anons and the Italian ``ha'') and over what role algorithmis should have in directly reverting human actions (e.g. PatruBOT and DexBot).  Because of ORES' open API and Wikipedia's collaborative wiki pages, it was relatively straightforward for Wikipedians to audit ORES' predictions and collaborate with each other to build information about trends in ORES' mistakes and how they expected their own processes to function.

\subsection{Future work}
One of the clear lines of future work that observing ORES in the world drives us toward is improved crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a very popular activity.  Despite the difficulty for a user of finding a deep wiki page and using templates to flag false positives, Wikipedians have managed to organize several similar processes for flagging false positives and calling them to our attention.  To better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structured way to report false positives and other real-human judgments, we can make it easy for tools that use ORES to also allow for reporting mistakes.  We can also make it easier to query a database of ORES mistakes in order to build the kind of thematic analyses that Italian Wikipedians showed us.  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and either get the model's predictions turned off or even shut down ORES.

We look forward to what those who work in the space of critical algorithm studies will do with ORES.  As of writing, most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on large for-profit organizations like Google and Facebook---organizations that can't afford to open up their proprietary algorithms due to competition.  Wikipedia is one of the largest and most important information resources in the world.  The algorithms that ORES makes available are part of the decision process for allowing some people to contribute and preventing other.  This is a context where \emph{algorithms matter to humanity}, and we are openly experimenting with the kind of transparent and open processes that the \emph{fairness and transparency} researchers are advocating.  Yet we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a real and important field site.

Finally, we also see potential in allowing Wikipedians, the denizens of Wikipedia, to freely train, test, and use their own prediction models.  Currently, ORES is only suited to deploy models that are developed by someone with a strong modeling and programming background.  That doesn't need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}}\footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted\_detection\_demo.ipynb}} and have found that beginning programmers can understand the work involved.  This is still not the holy grail of crowd developed machine prediction---where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would surface the relevant HCI issues involved and the changes the the technological conversations that such a margin-opening intervention might provide.
