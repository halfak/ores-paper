ORES as a socio-technical system has helped us 1) refine our understandings of volunteers' needs across wiki communities, 2) identify and address biases in ORES's models, and 3) reflect on how people think about what types of automation they find acceptable in their \emph{spaces}.  Through our participatory design process with various Wikipedian communities, we have arrived at several innovations in open machine learning practice that represent advancements in the field.

As we stated in Section~\ref{sec:design_rationale}, we measure success in new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be.  We have demonstrated through discussion of adoption patterns and case studies in reflection around the use of algorithmic systems that something fundamental is \emph{working}.  ORES is being heavily adopted.  The meaning of ORES models is being re-appropriated.  Both the models and the technologies that use the models are being collaboratively audited by their users and those who are affected.

\subsection{Participatory machine learning}
In a world increasingly dominated by for-profit content platforms --- often marketed by their corporate owners as ``communities'' \cite{gillespie2018custodians} --- Wikipedia is an anomaly. While the non-profit Wikimedia Foundation has only a fraction of the resources as Facebook or Google, the unique principles and practices in the broad Wikipedia/Wikimedia movement are a generative constraint. ORES emerged out of this context, operating at the intersection of a pressing need to deploy efficient machine learning at scale for content moderation, but to do so in ways that enable volunteers to develop and deploy advanced technologies on their own terms. Our approach is in stark contrast to the norm in machine learning research and practice, which involves a more top-down mode of developing the most precise classifiers for a known ground truth, then wrap those classifiers in a complete technology for end-users, who must treat them as black boxes.

The more wiki-inspired approach to what we call ``participatory machine learning'' imagines classifiers to be just as provisional and open to skeptical reinterpretation as the content of Wikipedia's encyclopedia articles. And like Wikipedia articles, we suspect some classifiers will be far better than others based on how volunteers develop and curate them, for various definitions of ``better'' that are already being actively debated. Our case studies briefly indicate how volunteers have collectively engaged in sophisticated discussions about how they ought to use machine learning. ORES' fully open, reproducible, and audit-able code and data pipeline---from training data to models to scored predictions---enables a wide range of new collaborative practices. ORES is a more socio-technical approach to issues in FATML, where attention is often placed on technical solutions, like interactive visualizations for model interpretability or mathematical guarantees of operationalized definitions of fairness. Our approach is specific to the particular practices and values of Wikipedia, and we have shown how ORES has been developed to fit into this context.

ORES also represents an innovation in openness in that it decouples several activities that have typically all been performed by engineers or under their direct supervision: choosing or curating training data, building models to serve predictions, auditing predictions for false positives/negatives, and developing interfaces or automated agents that act on those predictions. Often, those who develop and maintain the technical infrastructure for systems gain what we can call an \textit{incidental jurisdiction} over the other areas, which does not necessarily require that same expertise. As our cases have shown, people with extensive contextual and domain expertise in an area can make well-informed decisions about curating training data, identifying false positives/negatives, setting thresholds, and designing interfaces that use scores from a classifier. In decoupling these actions, ORES helps delegate these responsibilities more broadly, opening up the structure of the socio-technical system and expanding who can participate in it.

\subsection{Critical reflection}
In section~\ref{sec:case_studies}, we showed evidence of critical reflection on the current processes and the role of algorithms in quality control.  These case studies show that collaborative auditing is taking place, that there is a proliferation of tools based on alternative uses of ORES we did not imagine, and that that Wikipedians have more agency over their quality control processes. We also see an important expansion into supporting non-English language Wikipedias, which have historically not received as much support in this area. We are inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anon bias and the Italian ``ha'') and over what role algorithms should have in directly reverting human actions (e.g. PatruBOT and Dexbot).

Eliciting this type of critical reflection and empowering users to engage in their own choices about the roles of algorithmic systems in their social spaces has typically been more of a focus from the Critical Algorithms Studies literature, which comes from a more humanistic and interpretivist social science perspective (e.g. \cite{barocas2013governing, kitchin2017thinking}. This literature also emphasizes a need to see algorithmic systems as dynamic and constantly under revision by developers \cite{seaver2017algorithms} --- work that is invisible in most platforms, but is foregrounded in ORES. In these case studies, we see that given ORES' open API and Wikipedia's collaborative wiki pages, Wikipedians will audit ORES' predictions and collaborate with each other to build information about trends in ORES' mistakes and how they expected their own processes to function.

\subsection{Future work}
Observing ORES in practice suggests avenues of future work toward crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a popular activity.  Even though we did not design interfaces for discussion and auditing, some Wikipedians have used unintended affordances of wiki pages and MediaWiki's template system to organize processes for flagging false positives and calling them to our attention.  This process has proved invaluable for improving model fitness and addressing critical issues of bias against disempowered contributors.  To better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structured way to report what machine prediction gets right and wrong, we can make it easier for tools that use ORES to also allow for reporting mistakes and for others to infer trends.  For example, a database of ORES mistakes could be queried in order to build the kind of thematic analyses that Italian Wikipedians showed us (see section~\ref{sec:case_studies}).  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and either get the model's predictions turned off or even shut down the use of ORES (e.g. PatruBOT).

We also look forward to what those from the FATML and CAS fields can do with ORES, which is far more open than most high-scale machine learning applications. Most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on for-profit organizations that are strongly resistant to external interrogation. Wikipedia is one of the largest and arguably most impactful information resources in the world, and decisions about what is and is not represented have impacts across all sectors of society.  The algorithms that ORES makes available are part of the decision process that leads to some people's contributions remaining and others being removed.  This is a context where \emph{algorithms matter to humanity}, and we are openly experimenting with the kind of transparent and open processes that \emph{fairness and transparency in machine learning} researchers are advocating.  Yet, we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a specific and important field site.

Finally, we also see potential in allowing Wikipedians to freely train, test, and use their own prediction models without our engineering team involved in the process.  Currently, ORES is only suited to deploy models that are trained and tested by someone with a strong modeling and programming background, and we currently do that work for those who come to us with a training dataset and ideas about what kind of classifier they want to build.  That does not necessarily need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}} \footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted_detection_demo.ipynb}} and have found that new programmers can understand the work involved.  This is still not the holy grail of crowd-developed machine prediction, where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would surface the relevant HCI issues involved and the changes to the technological conversations that such a margin-opening intervention might provide.
