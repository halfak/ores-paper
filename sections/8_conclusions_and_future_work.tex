Designing for empowerment leads us in new directions.  Rather than running an experiment on an ``intervention'' (e.g. \cite{halfaker2014snuggle}, we're reducing barriers and encouraging a ``conversation'' that has stalled to continue.  We see this as a ``hearing to speech'' ala Nell Morten\cite{morton1985journey} in contrast to ``speaking to be heard''.  By building the exact technology or process we think is right, we would be ``speaking to be heard'' and forcing our own views into the technological conversation about how quality is enacted in Wikipedia.  However, by stepping back and asking, ``What is preventing others from getting involved in this conversation?'' and lowering those barriers, we run the risk that the conversation will not go in the directions that we value.  It is a conscious choice we make to attempt to empower others rather than to assert ourselves, but we do not make this choice purely out of altruism.  It is simply impractical for one individual or small group to drive a conversation in Wikipedia in a productive direction.  Halfaker et al.'s Snuggle intervention\cite{halfaker2014snuggle} tried that strategy with limited success in past work.  We think it's time to hear to speech and see what others would like to contribute from their own standpoints.

When considering our design rationale, ORES is not quite a successor system.  It doesn't directly enact an alternative vision of how quality control should function in Wikipedia  It is a system for making the construction of successor technologies easier.  By solving for efficiency and letting others design for process, new, expanded standpoints can more easily be expressed.  Under another view, ORES is maybe a successor in that it enacts an alternative view of how algorithms should be made available to the populations they govern.  By keeping algorithms integrated into specific tools and keeping the operational details of the algorithm hidden, past developers enacted values of control and focused use of their creations.  By opening ORES as an algorithmic system, encouraging experimentation, and providing tools for interrogation of the algorithms themselves, we enact a set of values that embrace experimentation and the progress of mediation.

Still we do intend to help Wikipedia's social process form in ways that align with our values---the more complete balance of efficient quality control and newcomer support.  We want to see Wikipedia work better---and by ``better'' we mean that more people who want to contribute will feel welcome to do so and will find their place in Wikipedia.  However, in this paper we provide no direct evaluation of newcomer retention nor do we look for evidence of improved newcomer socialization.  Instead, we're targeting the early precursors of social change: reflection on the current processes and the role of algorithms in quality control.  We believe that the case studies that we describe both show that this reflection is taking place and that the wide proliferation of tools that provide surprising alternative uses of ORES suggest that  Wikipedians feel a renewed power over the their quality control processes.  Even if we are wrong about the direction of change that Wikipedia needs for quality control and newcomer socialization, we're inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anons and the Italian ``ha'' from Section~\ref{sec:case_studies}) and the novel tools that volunteers are developing to support new editors (e.g. Ross's article quality recommender system from Section~\ref{sec:innovations_in_openness}).

\subsection{Future work}
One of the clear lines of future work that observing ORES in the world drives us toward is improved crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a very popular activity.  Despite the difficulty for a user of finding a deep wiki page and using templates to flag false positives, Wikipedians have managed to organize several similar processes for flagging false positives and calling them to our attention.  In order to better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structure way to report false positives and other real-human judgments, we can make it easy for tools that use ORES to also allow for reporting mistakes.  We can also make it easier to query a database of ORES mistakes in order to build the kind of thematic analyses that Italian Wikipedians showed us.  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and eventually either get the model's predictions turned off or even shut down ORES.

We look forward to what those who work in the space of critical algorithm studies will do with ORES.  As of writing, most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on large for-profit organizations like Google and Facebook---organizations that can't afford to open up their proprietary algorithms due to competition.  Wikipedia is one of the largest and most important information resources in the world.  The algorithms that ORES makes available are part of the decision process for allowing some people to contribute and preventing other.  This is a context where \emph{algorithms matter to humanity} and we are openly experimenting with the kind of transparent and open processes that the \emph{fairness and transparency} researchers are advocating.  Yet we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a real and important field site.

Finally, we also see potential in allowing Wikipedians, the denizens of Wikipedia, to freely train, test, and use their own prediction models.  Currently, ORES is only suited to deploy models that are developed by someone with a strong modeling and programming background.  However that doesn't need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}}\footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted_detection_demo.ipynb}} and have found that beginning programmers can understand the work involved.  This is still not the holy grail of crowd develop machine prediction---where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would be a very interesting exploration of both the HCI issues involved and the changes the the technological conversations that such a margin-opening intervention might provide.
