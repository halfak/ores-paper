ORES as a socio-technical system has helped us 1) refine our understandings of volunteers' needs across wiki communities, 2) identify and address biases in ORES's models, and 3) reflect on how people think about what types of automation they find acceptable in their \emph{spaces}.  Through our open participatory process with various Wikipedian communities, we have arrived at several innovations in open machine learning practice that represent advancements in the field.

As we stated in Section~\ref{sec:design_rationale}, we measure success in new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be.  We have demonstrated through discussion of adoption patterns and case studies in reflection around the use of algorithmic systems that something fundamental is \emph{working}.  ORES is being heavily adopted.  The meaning of ORES models is being re-appropriated.  Both the models and the technologies that use the models are being collaboratively audited by their users and those who are affected.

\subsection{Participatory machine learning}
In a world increasingly dominated by for-profit content platforms --- often marketed by their corporate owners as ``communities'' \cite{gillespie2018custodians} --- Wikipedia is an anomaly. While the non-profit Wikimedia Foundation has only a fraction of the resources as Facebook or Google, the unique principles and practices in the broad Wikipedia/Wikimedia movement are a generative constraint. ORES emerged out of this context, operating at the intersection of a pressing need to deploy efficient machine learning at scale for content moderation, but to do so in ways that enable volunteers to develop and deploy advanced technologies on their own terms. Our approach is in stark contrast to the norm in machine learning research and practice, which involves a more top-down mode of developing the most precise classifiers for a known ground truth, then wrap those classifiers in a complete technology for end-users, who must treat them as black boxes.

The more wiki-inspired approach to what we call ``participatory machine learning'' imagines classifiers to be just as provisional and open to skeptical reinterpretation as the content of Wikipedia's encyclopedia articles. And like Wikipedia articles, we suspect some classifiers will be far better than others based on how volunteers develop and curate them, for various definitions of ``better'' that are already being actively debated. Our case studies briefly indicate how volunteers have collectively engaged in sophisticated discussions about how they ought to use machine learning. ORES' fully open, reproducible, and audit-able code and data pipeline---from training data to models to scored predictions---enables a wide range of new collaborative practices. ORES is a more socio-technical and CSCW-oriented approach to issues in the FATML space, where attention is often placed on technical solutions, like interactive visualizations for model interpretability or mathematical guarantees of operationalized definitions of fairness.  Our approach is specific to the particular practices and values of Wikipedia, and we have shown how ORES has been developed to fit into this context.

ORES also represents an innovation in openness in that it decouples several activities that have typically all been performed by engineers or under their direct supervision: choosing or curating training data, building models to serve predictions, auditing predictions for false positives/negatives, and developing interfaces or automated agents that act on those predictions. Often, those who develop and maintain the technical infrastructure for systems gain what we can call an \textit{incidental jurisdiction} over the other areas, which does not necessarily require that same expertise. As our cases have shown, people with extensive contextual and domain expertise in an area can make well-informed decisions about curating training data, identifying false positives/negatives, setting thresholds, and designing interfaces that use scores from a classifier. In decoupling these actions, ORES helps delegate these responsibilities more broadly, opening up the structure of the socio-technical system and expanding who can participate in it.

\subsection{Critical reflection}
In section~\ref{sec:case_studies}, we showed evidence of critical reflection on the current processes and the role of algorithms in quality control.  These case studies show that collaborative auditing is taking place, that there is a proliferation of tools based on alternative uses of ORES we did not imagine, and that that Wikipedians have more agency over their quality control processes. We also see an important expansion into supporting non-English language Wikipedias, which have historically not received as much support in this area. We are inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anon bias and the Italian ``ha'') and over what role algorithms should have in directly reverting human actions (e.g. PatruBOT and Dexbot).

Eliciting this type of critical reflection and empowering users to engage in their own choices about the roles of algorithmic systems in their social spaces has typically been more of a focus from the Critical Algorithms Studies literature, which comes from a more humanistic and interpretivist social science perspective (e.g. \cite{barocas2013governing, kitchin2017thinking}. This literature also emphasizes a need to see algorithmic systems as dynamic and constantly under revision by developers \cite{seaver2017algorithms} --- work that is invisible in most platforms, but is foregrounded in ORES. In these case studies, we see that given ORES' open API and Wikipedia's collaborative wiki pages, Wikipedians will audit ORES' predictions and collaborate with each other to build information about trends in ORES' mistakes and how they expected their own processes to function.

\subsection{Design implications}
In many user-generated content communities (UGCs), the technologies that mediate social spaces are technically controlled by a single organization. Much work in exploring fairness in machine learning advocates decentralized services, but we have shown how if such an organization seeks to involve its users and stakeholders in the process, they can employ a \emph{decoupling} strategy like that of ORES, where the infrastructural expertise needed to build and serve ML models at scale is managed by the professionals, while other stakeholders curate training data, audit model performance, and decide where and how the ML models will be used. The case studies show the feasibility and benefits of decoupling the ML modeling service from the curation of training data and the implementation of ML scores in interfaces and tools, as well as in moving away from a single ``one classifier to rule them all'' and towards giving users the ability to train and serve models. 

We show how with such a service, non-technical users can and do play critical roles in the governance of ML in Wikipedia, which go beyond what they would be capable of if ORES were simply another ML classifier hidden behind a single-purpose UI albeit with open-source code and training data (which is what prior ML models in Wikipedia were). Because ORES is a service in production designed to let others more easily train and deploy their own models, this achieves a critical decoupling between algorithmic prediction and the intended use -- thus encouraging and enabling re-appropriation.  This strategy reduces incidental complexities involved in developing the systems necessary for deploying ML in production and at scale and turns that ML into a common resource on which others can audit and make use of separately from any specific implementation.

Specifically, our design implications for UGCs is to take this literally: to run open ML as a service platforms so that users can build their own models with training datasets they provide, which serve predictions using open APIs, and support activities like \emph{feature injection} and \emph{threshold optimization} for auditing and re-appropriation. Together with a common set of discussion spaces like the ones Wikipedians used, these can enable the re-use of models by a broader audience and encourage/make space for reflective practices such as model auditing, decision-making about thresholds, or the choice between different classifiers trained on different training datasets.  Our work shows that that non-technical users (at least in our field site) have an interest in participating in such governance activities and that such users can effectively coordinate common understandings of what a machine learning model is doing and whether or not that is acceptable to them.

\subsection{Future work}
Observing ORES in practice suggests avenues of future work toward crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a popular activity.  Even though we did not design interfaces for discussion and auditing, some Wikipedians have used unintended affordances of wiki pages and MediaWiki's template system to organize processes for flagging false positives and calling them to our attention.  This process has proved invaluable for improving model fitness and addressing critical issues of bias against disempowered contributors.  To better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structured way to report what machine prediction gets right and wrong, we can make it easier for tools that use ORES to also allow for reporting mistakes and for others to infer trends.  For example, a database of ORES mistakes could be queried in order to build the kind of thematic analyses that Italian Wikipedians showed us (see section~\ref{sec:case_studies}).  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and either get the model's predictions turned off or even shut down the use of ORES (e.g. PatruBOT).

We also look forward to what those from fairness in MN and critical algorithm studies can do with ORES, which is far more open than most high-scale machine learning applications. Most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on for-profit organizations that are strongly resistant to external interrogation. Wikipedia is one of the largest and arguably most impactful information resources in the world, and decisions about what is and is not represented have impacts across all sectors of society.  The algorithms that ORES makes available are part of the decision process that leads to some people's contributions remaining and others being removed.  This is a context where \emph{algorithms matter to humanity}, and we are openly experimenting with the kind of transparent and open processes that \emph{fairness and transparency in machine learning} researchers are advocating.  Yet, we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a specific and important field site.

Finally, we also see potential in allowing Wikipedians to freely train, test, and use their own prediction models without our engineering team involved in the process.  Currently, ORES is only suited to deploy models that are trained and tested by someone with a strong modeling and programming background, and we currently do that work for those who come to us with a training dataset and ideas about what kind of classifier they want to build.  That does not necessarily need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}} \footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted_detection_demo.ipynb}} and have found that new programmers can understand the work involved.  This is still not the holy grail of crowd-developed machine prediction, where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would surface the relevant HCI issues involved and the changes to the technological conversations that such a margin-opening intervention might provide.
