\subsection{Participatory machine learning}
In a world dominated by for-profit social computing and user-generated content platforms---often marketed by their corporate owners as ``communities''\cite{gillespie2018custodians}---Wikipedia is an anomaly. While the non-profit Wikimedia Foundation has only a fraction of the resources as Facebook or Google, the unique principles and practices in the broad Wikipedia/Wikimedia movement are a generative constraint. ORES emerged out of this context, operating at the intersection of a pressing need to deploy efficient machine learning at scale for content moderation, but to do so in ways that enable volunteers to develop and deploy advanced technologies on their own terms. Our approach is in stark contrast from the norm in machine learning research and practice, which involves a more top-down mode of developing the most precise classifiers for a known ground truth, then deploying a complete technology for end-users, who must treat them as black boxes.

The more wiki-inspired approach to what we call ``participatory machine learning'' ORES supports imagines classifiers to be just as provisional and open to skeptical reinterpretation as the content of Wikipedia's encyclopedia articles. And like Wikipedia articles, we suspect some classifiers will be far better than others based on how volunteers develop and curate them, for various definitions of ``better'' that are already being actively debated. Our case studies briefly indicate how volunteers have collectively engaged in sophisticated discussions about how they ought to use machine learning. ORES' fully open, reproducible, and auditable code and data pipeline---from training data to models to scored predictions---enables a wide and rich range of new collaborative practices. We see ORES as a more socio-technical and specifically CSCW approach to issues around fairness, accountability, and transparency in machine learning, where much attention is placed on technical solutions, like interactive visualizations for model interpretability or mathematical guarentees of different operationalized definitions of fairness. Our approach is specific to the particular genre ecology, work practices, and values of Wikipedia, and we have shown how ORES has been developed to fit into this complex socio-technical system.

\subsection{``Hearing to speech'': lowering barriers to participation}
Our goal to design for empowerment lead us in new directions -- both from traditional machine learning work more generally and in specific responses to the problems around automation in Wikipedia. Rather than making our own ``intervention'' based on our own ideas about how to tackle these problems (e.g. \cite{halfaker2014snuggle}), our approach focused on reducing barriers and encouraging a stalled ``conversation.'' As Bowker and Star discuss when considering participatory design for margins\cite{bowker1999sorting}, we see our work on ORES as closer to Nell Morten's concept of ``hearing to speech'' in contrast to ``speaking to be heard\cite{morton1985journey}''. If we were to build the exact technology or process we think is right for Wikipedia, we would be ``speaking to be heard'' and forcing our own solution-focused ideas into the conversation about how quality is enacted in Wikipedia. We instead stepped back and asked ``What is preventing others from getting involved in this conversation?'' then sought to lower those barriers.

ORES does enact our own alternative vision of how quality control should operate in Wikipedia, but only at an indirect meta-level by reflecting a different way of how algorithms should be made available to the populations they govern. Prior ML-based automated tools and bots in Wikipedia often involved a single developer producing both a classifier and an interface or bot that acted on that classifier, coupling these distinct aspects and requiring far more skills and resources if a volunteer wanted to modify or fork an existing tool or bot. There are still many barriers that remain, as creating an ORES classifier remains a complex technical task, and classifiers are not yet as radically open to live editing and discussion as a standard Wikipedia article. However, we have seen firsthand how ORES has supported new kinds of complex conversations and collaborations between volunteer tool developers and Wikimedia Foundation technical staff (or volunteer tool developers and their community of editors).

In taking such a decentralized approach, we run the risk that the conversation (and the processes adaptations) will not go in the directions that we value. Like with Wikipedia's articles, ORES's classifiers are used by self-selected individuals who have the means and motivation to participate, and research has repeatedly showed the deep demographic inequalities in participation\cite{lam2011wp,graham2014uneven}. Yet in this existing situation, we make a conscious choice to design for lowered technical barriers, as well as to foster more legitimate modes of peripheral participation. This more decentralized approach is also a necessity when working within the unique community governance model behind Wikipedia.  These generative constraints demanded that ORES would be built as a meta-algorithmic system, encouraging experimentation, discussion, and re-use, as well providing tools for interrogation of the classifiers themselves.

We also note that any system which expands participation brings with it the potential for expanded abuse and harassment, and while we have no firm solutions, we are mindful of such issues and recommend that future work in participatory machine learning be mindful as well. We also do not pretend to be completely value-neutral, as we acknowledge we want to help Wikipedia's social processes form in ways that align with our values, which include the more complete balance of efficient quality control and newcomer support. We want to see Wikipedia work better, and for us, ``better'' means that more people who want to contribute will feel welcome to do so and will find their place in Wikipedia. On this goal, this paper provides no direct evaluation of newcomer retention, nor do we look for evidence of improved newcomer socialization. Instead, we target the early precursors of social change: ecological health and critical reflection.

\subsection{Ecological health}
In section~\ref{sec:adoption_patterns}, we show clear evidence of a boom in the ecosystem of quality control technologies.  Through the adoption of ORES, new tools have quickly gained relevance.  Many of these tools re-mediate the way that quality control happens in Wikipedia.  For example, consider Sage Ross's use of ORES' interrogability to support newcomers in developing articles that Wikipedians would find acceptable.  By using a quality model to direct newcomers behavior, socialization may be improved at no cost to efficiency---maybe even an improvement from a reviewer's point of view since the system would result in more structurally complete drafts.  This addition to the technological conversation around quality control represents a substantial shift from the focus on the post-edit boundary\cite{geiger2012defense} to a focus on pre-review training for newcomers.  While Sage's interface has yet to be widely adopted, it represents a key example of a new concept in the technological conversation in Wikipedia.

While ORES has been live for 3 years at the time of writing, the ecological intervention is still young.  "ORES" has only recently become commonly known by Wikimedia contributors and we are still building out basic support for the hundreds of different Wikimedia language/project communities.  In order to see a fundimental shift in the Wikimedia's technical ecology, we'll likely need to continue observing for 1-2 more years still.  But the early signs of ecological health (heavy adoption and increased diversity of quality management technologies) are what we hope to see---long term and continual re-mediation of quality and community management policies, processes, and technologies.

\subsection{Critical reflection}
In section~\ref{sec:case_studies}, we show evidence of critical reflection on the current processes and the role of algorithms in quality control.  We believe that the case studies that we describe both show that this reflection is taking place and that the wide proliferation of tools that provide surprising alternative uses of ORES suggest that Wikipedians feel a renewed power over the their quality control processes.  Even if we are wrong about the direction of change that Wikipedia needs for quality control and newcomer socialization, we are inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anons and the Italian ``ha'') and over what role algorithmis should have in directly reverting human actions (e.g. PatruBOT and DexBot).

Eliciting this type of critical reflection and empowering users to engage in their own choices about the roles of algorithmic systems in their social spaces has typically been more of a focus from the Critical Algorithms Studies literature (e.g. \cite{barocas2013governing, kitchin2017thinking}. This literature also emphasizes a need to see algorithmic systems as dynamic and constantly under revision by developers \cite{seaver2017algorithms}---work that is invisible in most platforms, but foregrounded in ORES. In these case studies, we see that given ORES' open API and Wikipedia's collaborative wiki pages, Wikipedians will audit ORES' predictions and collaborate with each other to build information about trends in ORES' mistakes and how they expected their own processes to function.

\subsection{Future work}
One of the clear lines of future work that observing ORES in the world drives us toward is improved crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a very popular activity. Even though we did not design interfaces for discussion and auditing, some Wikipedians have used unintended affordances of wiki pages and MediaWiki's template system to organize several similar processes for flagging false positives and calling them to our attention.  To better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structured way to report false positives and other real-human judgments, we can make it easy for tools that use ORES to also allow for reporting mistakes.  We can also make it easier to query a database of ORES mistakes in order to build the kind of thematic analyses that Italian Wikipedians showed us.  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and either get the model's predictions turned off or even shut down ORES.

We also look forward to what future work in the space of critical algorithm studies will do with ORES.  As of writing, most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on large for-profit organizations like Google and Facebook---organizations that can't afford to open up their proprietary algorithms due to competition.  Wikipedia is one of the largest and most important information resources in the world.  The algorithms that ORES makes available are part of the decision process that leads to some people's contributions staying and others being removed.  This is a context where \emph{algorithms matter to humanity}, and we are openly experimenting with the kind of transparent and open processes that \emph{fairness and transparency in machine learning} researchers are advocating.  Yet we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a real and important field site.

Finally, we also see potential in allowing Wikipedians, the denizens of Wikipedia, to freely train, test, and use their own prediction models without our engineering team involved in the process.  Currently, ORES is only suited to deploy models that are trained and tested by someone with a strong modeling and programming background.  That doesn't need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}}\footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted_detection_demo.ipynb}} and have found that beginning programmers can understand the work involved.  This is still not the holy grail of crowd developed machine prediction---where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would surface the relevant HCI issues involved and the changes the the technological conversations that such a margin-opening intervention might provide.
