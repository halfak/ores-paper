ORES as a socio-technical system has helped us 1) refine our understandings of volunteers' needs across wiki communities, 2) identify and address biases in ORES's models, and 3) reflect on how people think about what types of automation they find acceptable in their \emph{spaces}.  Through our participatory design process with various Wikipedian communities, we've arrived at several innovations in open machine learning practice that represent advancements in the field.

As we stated in Section~\ref{sec:design_rationale}, we measure success in "new conversations about how algorithmic tools affect editing dynamics, as well as new types of tools that take advantage of these resources, implementing alternative visions of what Wikipedia is and ought to be."  We have demonstrated through discussion adoption patterns and case studies in reflection around the use of algorithmic systems that something fundamental is \emph{working}.  ORES is being heavily adopted.  The meaning of ORES models is being re-appropriated.  Both the models and the technologies that use the models are being collaboratively audited by their users and those who are affected.

\subsection{Participatory machine learning}
In a world dominated by for-profit social computing and user-generated content platforms---often marketed by their corporate owners as ``communities''\cite{gillespie2018custodians}---Wikipedia is an anomaly. While the non-profit Wikimedia Foundation has only a fraction of the resources as Facebook or Google, the unique principles and practices in the broad Wikipedia/Wikimedia movement are a generative constraint. ORES emerged out of this context, operating at the intersection of a pressing need to deploy efficient machine learning at scale for content moderation, but to do so in ways that enable volunteers to develop and deploy advanced technologies on their own terms. Our approach is in stark contrast to the norm in machine learning research and practice, which involves a more top-down mode of developing the most precise classifiers for a known ground truth, then wrap those classifiers in a complete technology for end-users, who must treat them as black boxes.

The more wiki-inspired approach to what we call ``participatory machine learning'' imagines classifiers to be just as provisional and open to skeptical reinterpretation as the content of Wikipedia's encyclopedia articles. And like Wikipedia articles, we suspect some classifiers will be far better than others based on how volunteers develop and curate them, for various definitions of ``better'' that are already being actively debated. Our case studies briefly indicate how volunteers have collectively engaged in sophisticated discussions about how they ought to use machine learning. ORES' fully open, reproducible, and auditable code and data pipeline---from training data to models to scored predictions---enables a wide range of new collaborative practices. ORES is a more socio-technical approach to issues in FATML, where attention is often placed on technical solutions, like interactive visualizations for model interpretability or mathematical guarantees of operationalized definitions of fairness. Our approach is specific to the particular practices and values of Wikipedia, and we have shown how ORES has been developed to fit into this context.

\subsection{Critical reflection}
In section~\ref{sec:case_studies}, we show evidence of critical reflection on the current processes and the role of algorithms in quality control.  We believe that the case studies that we describe both show that collaborative auditing is taking place and that the wide proliferation of tools that provide surprising alternative uses of ORES suggest that Wikipedians feel a renewed power over their quality control processes.  We are inspired by much of the concern that has surfaced for looking into biases in ORES' prediction models (e.g. anon bias and the Italian ``ha'') and over what role algorithms should have in directly reverting human actions (e.g. PatruBOT and DexBot).

Eliciting this type of critical reflection and empowering users to engage in their own choices about the roles of algorithmic systems in their social spaces has typically been more of a focus from the Critical Algorithms Studies literature (e.g. \cite{barocas2013governing, kitchin2017thinking}. This literature also emphasizes a need to see algorithmic systems as dynamic and constantly under revision by developers \cite{seaver2017algorithms}---work that is invisible in most platforms, but foregrounded in ORES. In these case studies, we see that given ORES' open API and Wikipedia's collaborative wiki pages, Wikipedians will audit ORES' predictions and collaborate with each other to build information about trends in ORES' mistakes and how they expected their own processes to function.

\subsection{Future work}
Observing ORES in practice suggests avenues of future work toward crowd-based auditing tools.  As our case studies suggest, auditing of ORES' predictions and mistakes has become a very popular activity.  Even though we did not design interfaces for discussion and auditing, some Wikipedians have used unintended affordances of wiki pages and MediaWiki's template system to organize similar processes for flagging false positives and calling them to our attention.  This process has proved invaluable for improving model fitness and addressing critical issues of bias against disempowered contributors.  To better facilitate this process, future system builders should implement structured means to refute, support, discuss, and critique the predictions of machine models.  With a structured way to report what machine prediction gets right and wrong, we can make it easier for tools that use ORES to also allow for reporting mistakes and for others to infer trends.  For example, a database of ORES mistakes could be queried in order to build the kind of thematic analyses that Italian Wikipedians showed us.  By supporting such an activity, we are working to transfer more power from ourselves and to our users.  Should one of our models develop a nasty bias, our users will be more empowered to coordinate with each other, show that the bias exists and where it causes problems, and either get the model's predictions turned off or even shut down ORES (e.g. PatruBOT).

We also look forward to what those in critical algorithm studies can do with ORES. Most of the studies and critiques of \emph{subjective algorithms}\cite{tufekci2015algorithms} focus on for-profit organizations that are strongly resistant to opening up. Wikipedia is one of the largest and most important information resources in the world.  The algorithms that ORES makes available are part of the decision process that leads to some people's contributions remaining and others being removed.  This is a context where \emph{algorithms matter to humanity}, and we are openly experimenting with the kind of transparent and open processes that \emph{fairness and transparency in machine learning} researchers are advocating.  Yet, we have new problems and new opportunities.  There is a large body of work exploring how biases manifest and how unfairness can play out in algorithmically mediated social contexts.  ORES would be an excellent place to expand the literature within a real and important field site.

Finally, we also see potential in allowing Wikipedians, the denizens of Wikipedia, to freely train, test, and use their own prediction models without our engineering team involved in the process.  Currently, ORES is only suited to deploy models that are trained and tested by someone with a strong modeling and programming background.  That doesn't need to be the case.  We have been experimenting with demonstrating ORES model building processes using Jupyter Notebooks\footnote{\url{http://jupyter.org}}\footnote{e.g. \url{ https://github.com/wiki-ai/editquality/blob/master/ipython/reverted_detection_demo.ipynb}} and have found that beginning programmers can understand the work involved.  This is still not the holy grail of crowd-developed machine prediction---where all of the incidental complexities involved in programming are removed from the process of model development and evaluation.  Future work exploring strategies for allowing end-users to build models that are deployed by ORES would surface the relevant HCI issues involved and the changes the the technological conversations that such a margin-opening intervention might provide.
