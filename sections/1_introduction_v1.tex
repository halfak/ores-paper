\todo{First few paragraphs are more like a talk intro than a paper intro.}

Imagine a space where algorithms are developed and deployed through inclusive, transparent, community-led documentation practices.  In such a place, what kind of dynamics would we expect to see?

Based on a reading of the critical algorithms literature, you'd think that we're just beginning to explore what such a space would look like.  Government and big, corporate ``platforms'' dominate the discussion of algorithmic governance issues\cite{crawford2016algorithm}\cite{diakopoulos2015algorithmic}\cite{gillespie2014relevance}\cite{tufekci2015algorithms}.

What if we told you that the Wikipedia\footnote{\url{https://wikipedia.org}} community has been developing algorithms---of the scary subjective type---in the open for more than a decade?  And, that there's an extensive literature about how the social and technical governance subsystems have developed in parallel.  In this field site, we see a unique set of new problems and opportunities that have yet to be encountered by the Facebooks, Twitters, and Googles of the internet.

By examining Wikipedia's algorithmic practices, we can explore research questions that are only askable after algorithmic development and adoption have become mostly \emph{inclusive, transparent, and community-led}.

This paper discusses Wikipedia as a state-of-the-art socio-technical space for human-process and algorithmic co-development through the case of the Objective Revision Evaluation Service (ORES)\footnote{\url{https://ores.wikimedia.org}}\footnote{\url{http://enwp.org/:mw:ORES}}), a machine learning service that supports community-curated classifiers.  ORES is novel as both a critical intervention and a technical probe\cite{hutchinson2003technology} into Wikipedia's socio-technical problems around algorithmic transparency and crowd-sourced auditing. The design rationale behind ORES draws from empirical research into Wikipedia's socio-technical problems and theoretical literatures including critical feminist theory and genre ecologies.  We discuss how these design rationales play out in the operation of the system and through case studies, where we show how independent communities of volunteers have responded to these AIs.

Since ORES's deployment in early 2015, it has been widely used by Wikipedians and discussed in scholarly literatures.  Before it was enabled by default, the ORES service was manually enabled by roughly half of the population of Wikipedia editors across all languages.  Our announcement blog post\cite{halfaker2015artificial} and the ORES URL have been cited and footnoted several times across research fields, used as a basis for improved modeling work (e.g. \cite{dang2016quality}), as an example of a publicly available machine prediction API (e.g. \cite{lewoniewski2017relative}), and through using the prediction models to support other analyses (e.g. \cite{rezguia2017stigmergic}).

By detailing ORES and discussing new technological and social interventions that ORES elicits, we open the doors for researchers to further build on this research platform\cite{terveen2014study}.  Through case studies of emergent reflection and auditing of ORES, we bring some constructive, empirically-grounded, and practical considerations to recent calls for the audit-ability of algorithms\cite{sandvig2014auditing}.

We first review related literature around open algorithmic systems, then discuss the socio-technical context of Wikipedia and the design rationale that lead us to building ORES.  Next, we describe how we engineered the ORES system to match Wikipedian work practices -- including innovations we've made with regards to algorithmic \emph{openness} and \emph{transparency}.  Finally, we present case studies of interesting uses and critiques of ORES' predictions.
