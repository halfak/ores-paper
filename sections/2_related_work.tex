\subsection{The politics of algorithms}
Algorithmic systems play increasingly cruical roles in the governance of social processes \cite{gillespie2014relevance}.  In online spaces, these systems help us deal with information overload problems: What search results best balance \emph{relevance} and \emph{importance}?  Which books are most \emph{related} to the ones a user likes?  In other spaces, algorithmic systems help institutions run more efficiently: Who is least \emph{risky} to loan money to?  Where should police patrol to encounter the most crime?  Software algorithms are increasingly used in answering such questions that have no single right answer and where prior human decisions used as training data can be problematic \cite{barocas2013governing,tufekci2015algorithms}.

Algorithms designed to support work change work practices, shifting the dynamics of that work\cite{crawford2016algorithm, gillespie2014relevance, zuboff1988age}.  Software algorithms gain political relevance on par with other process-mediating artifacts (e.g. laws\cite{lessig1999code}).  This increasing relevance of algorithms in social and political life renewed focus on questions of fairness and transparency\footnote{See also \url{https://www.fatml.org/} for a conference devoted to these questions}.

There are repeated calls to address power dynamics at play in algorithmic bias through transparency and accountability of the algorithms that govern public life and access to resources\cite{diakopoulos2017algorithmic}\cite{sandvig2014auditing}\cite{kroll2016accountable}.  The field around effective transparency and accountability mechanisms is growing.  We cannot fully address the scale of concerns in this rapidly shifting, but we find inspiration in Kroll et al's discussion of the potential and limitations of auditing and transparency.\cite{kroll2016accountable}

This paper discusses a specific political context (Wikipedia's algorithmic quality control and socialization practices) and the development of novel algorithms for support of these processes.  We implement an algorithmic intervention using the unusual strategy of deploying a set of prediction algorithms as a service, leaving decisions about appropriation to our users and other technology developers.  Instead of seeking to train the single best classifier and implement it in our own designs, we embrace public auditing and re-interpretations of our model's predictions as an \emph{intended} and \emph{desired} outcome.  Extensive work on technical and social ways to achieve fairness and accountability generally do not discuss this kind of infrastructural intervention on communities of practice and their articulation work.

\subsection{Machine prediction in support of open production}
Open peer production systems have a long history of using machine learning in service of efficiency. For Wikipedia and related Wikimedia projects, vandalism detection and quality control has been paramount for practitioners and researchers.  Article quality prediction models have also been explored and applied to help Wikipedians focus their work in the most beneficial places.

\leadin{Vandalism detection} The damage detection problem in Wikipedia is one of great scale.  English Wikipedia receives about 160,000 new edits every day, which immediately go live without review.  Wikipedians embrace this risk as the nature of an open encyclopedia, but work tirelessly to maintain quality. Every damaging or offensive edit puts the credibility of the community at risk, so all edits must be reviewed as soon as possible.

As an information overload problem, filtering strategies have been hugely successful in Wikipedia, and we briefly review a decade of work in this area. Potthast et al.'s seminal paper\cite{potthast2008automatic} describes a strategy using logistic regressions to automatically detecting vandalism. Jacobi described \emph{ClueBot}\cite{carter2008cluebot}, a Wikipedia editing bot deployed to automatically revert obvious vandalism. Adler et al. (2011) summarized the state of the art in feature extraction and modeling strategies, building a classifier that aggregated all features extraction strategies \cite{adler2011wikipedia}.  In some cases, researchers directly integrated their prediction models into tools for Wikipedians to use (e.g. STiki\cite{west2010stiki}, a machine-supported human-computation tool). Yet most of this modeling work remains in the literature and has not been incorporated into current tools in Wikipedia.

Wikipedians have built and orchestrated a complex, multi-stage filter for incoming edits that is efficient and effective.  Geiger \& Halfaker quantitatively describe the temporal rhythm of edit review in Wikipedia\cite{geiger2013levee}: First, automated bots revert vandalism that scores very highly according to a machine prediction model. Then ``vandal fighters'' use human-in-the-loop tools, where they review edits scored highly by machine prediction models, but not high enough for fully-automated reversion \cite{geiger2010work}.  Edits that make it past these bots and ``cyborgs''\cite{halfaker2012bots} are routed through a system of \emph{watchlists}\footnote{\url{http://enwp.org/:mw:Help:Watchlist}} to experienced Wikipedia editors who are interested in the articles being edited.  With this system in place, most damaging edits are reverted within seconds of when they are saved\cite{geiger2013levee} and Wikipedia is kept clean.

\leadin{Task routing} Task routing in Wikipedia is supported by a natural dynamic: people read what they are interested in, and when they see an opportunity to contribute, they do.  This leads to a demand-driven contribution pattern where the most viewed content tends to be edited to the highest quality\cite{hill2014consider}.  There are still many cases where Wikipedia remains misaligned\cite{wang2015misalignment}, and content coverage biases creep in (e.g. for a long period of time, the coverage of Women Scientists in Wikipedia lagged far behind the rest of the encyclopedia\cite{halfaker2017interpolating}).  By aligning interests with missed opportunities for contribution, these misalignments and gaps can be re-aligned and filled.  Past work has explored collaborative recommender-based task routing strategies (see SuggestBot\cite{cosley2007suggestbot}) and shown good success.  Recently, the maintainers of SuggestBot have developed article quality prediction models to help route attention to low quality articles\cite{wang2013tell}.  Warncke-Wang and Halfaker have also used the article quality model to perform some one-off analyses to help Wikipedians critique and update their own manual quality assessments\cite{wang2014screening}.

\subsection{The Rise and Decline: Wikipedia's socio-technical problems}
While Wikipedians have made significant succcess in the algorithmic systems developed to support quality control, a line of critical research has studied the unintended consequences of this complex socio-technical system, particularly on newcomer socialization. \cite{halfaker2013rise}\cite{morgan2013tea}\cite{halfaker2014snuggle}.  In summary, Wikipedians struggled with the issues of scaling when the popularity of Wikipedia grew exponentially between 2005 and 2007\cite{halfaker2013rise}.  In response, they developed quality control processes and technologies that prioritized efficiency by using machine prediction models\cite{halfaker2014snuggle} and templated warning messages\cite{halfaker2013rise}.  This transformed newcomer socialization from a primarily human and welcoming activity to one that is more dismissive and impersonal\cite{morgan2013tea}.  This is a good example of the values of the designers being captured in the process and supporting infrastructure they developed\cite{halfaker2014snuggle}.  The efficiency of quality control work and the elimination of damage was considered extremely politically important, while the positive experience of newcomers was less politically important.  The result was a sudden and sustained decline in the retention of good-faith newcomers and a decline in the overall population of Wikipedia editors\cite{halfaker2013rise}.

After the effect of this trade-off was made clear, a number of initiatives were started in an effort to more effectively balance the needs for good community management with quality control efficiency.  For example, a newcomer help space (The Teahouse\cite{morgan2013tea}) was developed to provide a more welcoming and forgiving space for newcomers to ask questions and meet experienced Wikipedians.  A newcomer training game was developed and tested with financial support from the Wikimedia Foundation\cite{narayan2015effects}.  The Wikimedia Foundation also formed a product development team that was tasked with making changes to Wikipedia's user interfaces to increase newcomer retention\footnote{\url{http://enwp.org/:m:Growth team}}.  Most of these efforts did not show gains in newcomer retention under experimental conditions.  An exception is the Teahouse, where it was shown that intervening by inviting newcomers to participate in the question and answer space had a statistically significant benefit to long term newcomer retention\cite{morgan2018evaluating}.

Despite these targeted efforts and shifts in perception among some members of the Wikipedia community, the quality control process that was designed over a decade ago remains largely unchanged\cite{halfaker2014snuggle}.  The quality control systems that were dominant before the publication of the of the seminal ``Rise and Decline'' study in 2013\cite{halfaker2013rise} remain dominant today.  The regime of automatic reverting bots and semi-automated tools that conceptualize edits as ``good'' and ``bad'' remains in place and unchanged.  Notably, Halfaker et al. experimented with developing a novel reversal of the damage detection tools by re-appropriating a damage detection model to highlight good editors who were running into trouble in the quality control system\cite{halfaker2014snuggle}, but there has not been a significant shift in how quality control is enacted in Wikipedia.
