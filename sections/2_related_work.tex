\subsection{The politics of algorithms}
Algorithmic systems play increasingly crucial roles in the governance of social processes \cite{gillespie2014relevance}. Software algorithms are increasingly used in answering questions that have no single right answer and where using prior human decisions as training data can be problematic \cite{barocas2013governing}. Algorithms designed to support work change people's work practices, shifting how, where, and by whom work is accomplished \cite{crawford2016algorithm, zuboff1988age}. Software algorithms gain political relevance on par with other process-mediating artifacts (e.g. laws, norms \cite{lessig1999code}).

There are repeated calls to address power dynamics and bias through transparency and accountability of the algorithms that govern public life and access to resources \cite{diakopoulos2017algorithmic,sandvig2014auditing}. The field around effective transparency, explainability, and accountability mechanisms is growing. We cannot fully address the scale of concerns in this rapidly shifting literature, but we find inspiration in Kroll et al's discussion of the limitations of auditing and transparency \cite{kroll2016accountable}, Mulligan et al's shift towards the term ``contestability'' \cite{mulligan2019shaping}, and Geiger's call to go ``beyond opening up the black box\cite{geiger2017beyond}''.

In this paper, we discuss a specific socio-political context---Wikipedia's algorithmic quality control and socialization practices---and the development of novel algorithmic systems for support of these processes.  We implement a meta-algorithmic intervention aligned with Wikipedians' principles and practices: deploying a set of prediction algorithms as a service and leaving decisions about appropriation to the volunteer community.  Instead of training the single best classifier and implementing it in our own designs, we embrace public auditing, re-interpretations, and appropriations of our models' predictions as an \emph{intended} and \emph{desired} outcome.  Extensive work on technical and social ways to achieve fairness and accountability generally do not discuss this kind of socio-infrastructural intervention on communities of practice.

\subsection{Machine prediction in support of open production}
Open peer production systems, like all user-generated content platforms, have a long history of using machine learning for content moderation and task management. For Wikipedia and related Wikimedia projects, vandalism detection and quality control is a major goal for practitioners and researchers.  Article quality prediction models have also been explored and applied to help Wikipedians focus their work in the most beneficial places.

\leadin{Vandalism detection} The damage detection problem in Wikipedia is one of great scale.  English Wikipedia receives about 160,000 new edits every day, which immediately go live without review.  Wikipedians embrace this risk as the nature of an open encyclopedia, but work tirelessly to maintain quality. Every damaging or offensive edit puts the credibility of the community and their product at risk, so all edits must be reviewed as soon as possible \cite{geiger2010work}. As an information overload problem, filtering strategies using machine learning models have been developed to support the work of Wikipedia's patrollers (see \cite{adler2011wikipedia} for an overview).  In some cases, researchers directly integrated their prediction models into purpose-designed tools for Wikipedians to use (e.g. STiki \cite{west2010stiki}, a classifier-supported human-computation tool). Through these machine learning models and constant patrolling, most damaging edits are reverted within seconds of when they are saved \cite{geiger2013levee}.

\leadin{Task routing and recommendation}
Machine learning plays a major role in how Wikipedians decide what articles to work on, supplementing the standard self-selected dynamic of people contributing to topics they are interested in. Wikipedia has many well-known content coverage biases (e.g. for a long period of time, the coverage of women scientists in Wikipedia lagged far behind the rest of the encyclopedia \cite{halfaker2017interpolating}). Past work has explored collaborative recommender-based task routing strategies (see SuggestBot \cite{cosley2007suggestbot}), in which contributors are sent articles that need improvement in their areas of expertise. Such systems show strong promise to address content coverage biases, but could also inadvertently reinforce biases.

\subsection{The Rise and Decline: Wikipedia's socio-technical problems}
While Wikipedians have successfully deployed algorithmic quality control support systems to maintain Wikipedia, a line of critical research has studied the unintended consequences of this complex socio-technical system, particularly on newcomer socialization~\cite{halfaker2013rise,morgan2013tea,halfaker2014snuggle}.  In summary, Wikipedians struggled with the issues of scaling when the popularity of Wikipedia grew exponentially between 2005 and 2007~\cite{halfaker2013rise}.  In response, they developed quality control processes and technologies that prioritized efficiency by using machine prediction models~\cite{halfaker2014snuggle} and templated warning messages~ \cite{halfaker2013rise}.  This transformed newcomer socialization from a primarily human and welcoming activity to one that is more dismissive and impersonal~\cite{morgan2013tea} and has caused in a steady decline in Wikipedia's editing population.  The efficiency of quality control work and the elimination of damage was considered extremely politically important, while the positive experience of newcomers was less so. After the research about this systemic issue came out, the political importance of newcomer experience was raised substantially.  But despite targeted efforts and shifts in perception among some members of the Wikipedia community~\cite{narayan2015effects, morgan2013tea}\footnote{See also a team dedicated to supporting newcomers\url{http://enwp.org/:m:Growth team}}, the quality control processes that were designed over a decade ago remain largely unchanged~\cite{halfaker2014snuggle}.

Wikipedian tool developers play a critical role in the larger conversation of how the community should be doing quality control~\cite{geiger2014bots, halfaker2014snuggle}. There are few formal barriers to anyone auditing, modifying, or developing their own classifier: there is substantial transparency both in open licensing of code and data, as well as an open and public governance model. However, Wikipedia's massive scale means that significant computational and data engineering expertise are required do so. This limits the types of people who are able to participate in the technological side of such a conversation.  Historically, tool developers who were motivated and capable of developing such tools chose to optimize for efficiency to the exclusion of other goals~\cite{halfaker2014snuggle}.  Today, we know that many of these other goals --- especially those related to newcomer retention and the diversity of contributors\footnote{\url{https://meta.wikimedia.org/wiki/Strategy/Wikimedia_movement/2017/Direction}} --- are also important~\cite{morgan2013tea, halfaker2013rise}.  In this paper, we describe a system that is designed to open up the technical side of this quality control conversation. Our aim is to allow a more diverse set of values to be represented, and for these values of the broader community to be more coherently expressed.