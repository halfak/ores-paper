\subsection{The politics of algorithms}
Algorithmic systems play increasingly crucial roles in the governance of social processes\cite{gillespie2014relevance}. Software algorithms are increasingly used in answering questions that have no single right answer and where prior human decisions used as training data can be problematic \cite{barocas2013governing}. Algorithms designed to support work change people's work practices, shifting how, where, and by whom work is accomplished\cite{crawford2016algorithm, zuboff1988age}. Software algorithms gain political relevance on par with other process-mediating artifacts (e.g. laws\cite{lessig1999code}).

There are repeated calls to address power dynamics and bias through transparency and accountability of the algorithms that govern public life and access to resources\cite{diakopoulos2017algorithmic,sandvig2014auditing}. The field around effective transparency and accountability mechanisms is growing. We cannot fully address the scale of concerns in this rapidly shifting literature, but we find inspiration in Kroll et al's discussion of the potential and limitations of auditing and transparency\cite{kroll2016accountable} and Geiger's call to go ``beyond opening up the black box'' \cite{geiger2017beyond}.

We discuss a specific socio-political context -- Wikipedia's algorithmic quality control and socialization practices -- and the development of novel algorithmic systems for support of these processes.  We implement a meta-algorithmic intervention aligned with Wikipedians' principles and practices: deploying a set of prediction algorithms as a service and leaving decisions about appropriation to the volunteer community.  Instead of training the single best classifier and implementing it in our own designs, we embrace public auditing, re-interpretations, and appropriations of our models' predictions as an \emph{intended} and \emph{desired} outcome.  Extensive work on technical and social ways to achieve fairness and accountability generally do not discuss this kind of socio-infrastructural intervention on communities of practice.

\subsection{Machine prediction in support of open production}
Open peer production systems, like all user-generated content platforms, have a long history of using machine learning for content moderation and task management. For Wikipedia and related Wikimedia projects, vandalism detection and quality control is a major goal for practitioners and researchers.  Article quality prediction models have also been explored and applied to help Wikipedians focus their work in the most beneficial places.

\leadin{Vandalism detection} The damage detection problem in Wikipedia is one of great scale.  English Wikipedia receives about 160,000 new edits every day, which immediately go live without review.  Wikipedians embrace this risk as the nature of an open encyclopedia, but work tirelessly to maintain quality. Every damaging or offensive edit puts the credibility of the community and their product at risk, so all edits must be reviewed as soon as possible\cite{geiger2010work}.

As an information overload problem, filtering strategies using machine learning models have been developed to support the work of Wikipedia's patrollers (see \cite{adler2011wikipedia} for an overview).  In some cases, researchers directly integrated their prediction models into specific, purpose-designed tools for Wikipedians to use (e.g. STiki\cite{west2010stiki}, a classifier-supported human-computation tool). Through the use of these machine learning models and boundary patrolling, most damaging edits are reverted within seconds of when they are saved\cite{geiger2013levee}.

\leadin{Task routing and recommendation}
Machine learning plays a major role in how Wikipedians decide what articles to work on, supplementing the standard self-selected dynamic of people contributing to topics they are interested in. Wikipedia has many well-known content coverage biases (e.g. for a long period of time, the coverage of women scientists in Wikipedia lagged far behind the rest of the encyclopedia\cite{halfaker2017interpolating}). Past work has explored collaborative recommender-based task routing strategies (see SuggestBot\cite{cosley2007suggestbot}), in which contributors are sent articles that need improvement in their areas of expertise. Such systems show strong promise to address content coverage biases, but could also inadvertently reinforce biases.

\subsection{The Rise and Decline: Wikipedia's socio-technical problems}
While Wikipedians have successfully algorithmic quality control support systems to maintain Wikipedia, a line of critical research has studied the unintended consequences of this complex socio-technical system, particularly on newcomer socialization \cite{halfaker2013rise,morgan2013tea,halfaker2014snuggle}.  In summary, Wikipedians struggled with the issues of scaling when the popularity of Wikipedia grew exponentially between 2005 and 2007\cite{halfaker2013rise}.  In response, they developed quality control processes and technologies that prioritized efficiency by using machine prediction models\cite{halfaker2014snuggle} and templated warning messages\cite{halfaker2013rise}.  This transformed newcomer socialization from a primarily human and welcoming activity to one that is more dismissive and impersonal\cite{morgan2013tea} and cause in a steady decline in Wikipedia's editing population.  The efficiency of quality control work and the elimination of damage was considered extremely politically important, while the positive experience of newcomers was less politically important.

After the research about this systemic issue came out, the political importance of newcomer experience was raised substantially.  But despite targeted efforts and shifts in perception among some members of the Wikipedia community\cite{narayan2015effects, morgan2013tea}\footnote{See also a team dedicated to supporting newcomers\url{http://enwp.org/:m:Growth team}}, the quality control processes that were designed over a decade ago remains largely unchanged\cite{halfaker2014snuggle}.
