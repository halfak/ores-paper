\subsection{The politics of algorithms}
Algorithmic systems play increasingly crucial roles in the governance of social processes\cite{gillespie2014relevance}.  Software algorithms are increasingly used in answering such questions that have no single right answer and where prior human decisions used as training data can be problematic \cite{barocas2013governing,tufekci2015algorithms}. Algorithms designed to support work change people's work practices, shifting how, where, and by whom work is accomplished\cite{crawford2016algorithm, gillespie2014relevance, zuboff1988age}.  Software algorithms gain political relevance on par with other process-mediating artifacts (e.g. laws\cite{lessig1999code}).

There are repeated calls to address power dynamics and bias through transparency and accountability of the algorithms that govern public life and access to resources\cite{diakopoulos2017algorithmic,sandvig2014auditing}.  The field around effective transparency and accountability mechanisms is growing.  We cannot fully address the scale of concerns in this rapidly shifting literature, but we find inspiration in Kroll et al's discussion of the potential and limitations of auditing and transparency\cite{kroll2016accountable} and Geiger's call to go ``beyond opening up the black box'' \cite{geiger2017beyond}.

We discuss a specific socio-political context -- Wikipedia's algorithmic quality control and socialization practices -- and the development of novel algorithmic systems for support of these processes.  We implement a meta-algorithmic intervention aligned with Wikipedians' principles and practices: deploying a set of prediction algorithms as a service and leaving decisions about appropriation to the volunteer community.  Instead of training the single best classifier and implementing it in our own designs, we embrace public auditing, re-interpretations, and appropriations of our models' predictions as an \emph{intended} and \emph{desired} outcome.  Extensive work on technical and social ways to achieve fairness and accountability generally do not discuss this kind of socio-infrastructural intervention on communities of practice.

\subsection{Machine prediction in support of open production}
Open peer production systems, like all user-generated content platforms, have a long history of using machine learning for content moderation and task management. For Wikipedia and related Wikimedia projects, vandalism detection and quality control is a major goal for practitioners and researchers.  Article quality prediction models have also been explored and applied to help Wikipedians focus their work in the most beneficial places.

\leadin{Vandalism detection} The damage detection problem in Wikipedia is one of great scale.  English Wikipedia receives about 160,000 new edits every day, which immediately go live without review.  Wikipedians embrace this risk as the nature of an open encyclopedia, but work tirelessly to maintain quality. Every damaging or offensive edit puts the credibility of the community and their product at risk, so all edits must be reviewed as soon as possible\cite{geiger2010work}.

As an information overload problem, filtering strategies using machine learning models have been developed to support the work of Wikipedia's patrollers (see \cite{adler2011wikipedia} for an overview).  In some cases, researchers directly integrated their prediction models into specific, purpose-designed tools for Wikipedians to use (e.g. STiki\cite{west2010stiki}, a classifier-supported human-computation tool). Through the use of these machine learning models and boundary patrolling, most damaging edits are reverted within seconds of when they are saved\cite{geiger2013levee}.

\leadin{Task routing}
Task routing in Wikipedia is supported by a natural dynamic: people read what they are interested in, and when they see an opportunity to contribute, they do.  This leads to a demand-driven contribution pattern where the most viewed content tends to be edited to the highest quality\cite{hill2014consider}.  There are still many cases where Wikipedia remains misaligned\cite{wang2015misalignment}, and content coverage biases creep in (e.g. for a long period of time, the coverage of women scientists in Wikipedia lagged far behind the rest of the encyclopedia\cite{halfaker2017interpolating}).  By aligning interests with missed opportunities for contribution, these misalignments and gaps can be re-aligned and filled.  Past work has explored collaborative recommender-based task routing strategies (see SuggestBot\cite{cosley2007suggestbot}), which show good success.  Recently, the maintainers of SuggestBot have developed article quality prediction models to help route attention to important, but low quality articles\cite{wang2013tell}.  Warncke-Wang and Halfaker have also used the article quality model to perform some one-off analyses to help Wikipedians critique and update their own manual quality assessments\cite{wang2014screening}.

\subsection{The Rise and Decline: Wikipedia's socio-technical problems}
While Wikipedians have successfully algorithmic quality control support systems to maintain Wikipedia, a line of critical research has studied the unintended consequences of this complex socio-technical system, particularly on newcomer socialization \cite{halfaker2013rise,morgan2013tea,halfaker2014snuggle}.  In summary, Wikipedians struggled with the issues of scaling when the popularity of Wikipedia grew exponentially between 2005 and 2007\cite{halfaker2013rise}.  In response, they developed quality control processes and technologies that prioritized efficiency by using machine prediction models\cite{halfaker2014snuggle} and templated warning messages\cite{halfaker2013rise}.  This transformed newcomer socialization from a primarily human and welcoming activity to one that is more dismissive and impersonal\cite{morgan2013tea} and cause in a steady decline in Wikipedia's editing population.  The efficiency of quality control work and the elimination of damage was considered extremely politically important, while the positive experience of newcomers was less politically important.

After the research about this systemic issue came out, the political importance of newcomer experience was raised substantially.  But despite targeted efforts and shifts in perception among some members of the Wikipedia community\cite{narayan2015effects, morgan2013tea}\footnote{See also a team dedicated to supporting newcomers\url{http://enwp.org/:m:Growth team}}, the quality control processes that were designed over a decade ago remains largely unchanged\cite{halfaker2014snuggle}.
